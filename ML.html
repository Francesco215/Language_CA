<d-section id="abs">Abstract for ML researchers</d-section>
<p>
The complexity of generating a text with N words with modern machine learning algorithms such as the Transformer is O(N^2). This makes it computationally infeasible to generate coherent pieces of text as long as books. Self-organizing systems, on the other hand, have two big strenghts that transformers lack
</p>
<p>
    <ul>
        <li>
        Efficency: Self-organizing systems often exhibit efficient resource allocation and optimization of processes. The emergent patterns and structures that arise from local interactions can lead to efficient distribution of tasks, information, or resources, resulting in overall system efficiency
        </li>
        <li>
            Scalability: Self-organizing systems can often scale effectively. As the number of components increases the system can still organize itself without requiring additional centralized control. This property is valuable in various domains, such as computer networks, where scalability is crucial
        </li>
    </ul>
</p>
<p>Inspired from Biology and using tools from Statistical Physics we show that:</p>
<p>
    <ul>
        <li>
            Autoregressive models, no matter the architecture, the number of parameters, the window size, and the training procedure will never be able to generate text that is coherent at arbitrarily long distances.
        </li>
        <li>
            Transformers with attention matrices with a specifically crafted topology, can in principle be able to generate text that is coherent at arbitrarily long distances.
        </li>
    </ul>
</p>