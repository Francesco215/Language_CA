<!doctype html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=1200">
    <script async src="https://distill.pub/template.v2.js"></script>
    <!-- <link rel="stylesheet" href="website/style.css"> -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css" integrity="sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js" integrity="sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script >
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                // • rendering keys, e.g.:
                throwOnError : false
            });
        });
    </script>

</head>

<link rel="stylesheet" href="style.css">

<d-front-matter>
  <script type="text/json">{
    "title": "Topological requirements to self organization",
    "description": "1 or 2 short sentences to describe the thing",
    "authors": [
      {
        "author": "Francesco Sacco",
        "authorURL": "https://github.com/Francesco215",
        "affiliation": "Tufts University",
        "affiliationURL": "http://www.tufts.edu"
      }
      ]
  }</script>
</d-front-matter>

  <d-title> 
    <h1>Topological requirements to self organization</h1>
    <p>
        What do you need to have a structure emerge spontaneously?<br>
        How does it relate to intelligence?
    </p>
    
  </d-title>

  <script>
    console.log('asd')
    
    function showContent(target) {
      // Get all the div elements
      const divs = document.querySelectorAll('div[target_audience]');

      // Loop through each div
      divs.forEach(div => {
        if (div.getAttribute('target_audience').includes(target)) {
          // Display the selected div
          div.style.display = 'block';
        } else {
          // Hide the other divs
          div.style.display = 'none';
        }
      });
    }
  </script>

  <d-article>
    <div style="position:relative;" class="l-body">
    
      <div id="colab-hero-div" style="float:left;" >
        <div  class="colab-root" id="colab-hero" onclick="showContent('b')">I am a Biologist
        </div>
      </div>

      <div id="colab-hero-div" style="float:left;">
        <div  class="colab-root" id="colab-hero" onclick="showContent('m')">I am a ML researcher
        </div>
      </div>

      <div id="colab-hero-div"style="float:left;">
        <div  class="colab-root" id="colab-hero" onclick="showContent('p')">I am a Physicist
        </div>
      </div> 
    </div>
    <div style="padding:10px 10px ;"></div>
    <div>
      
    </div>
    <div target_audience="n" style="display: block;">
      <d-section id="abs">Adaptive Abstract</d-section>

      <p>
        This is an example of an interactive abstract. Different types of researcher will have different abstract summarizing what they care the most. Click on the buttons above to have your abstract
      </p>
    </div>
    <div target_audience="b" style="display: none;">
      <d-section id="abs">Abstract for Biologists</d-section>
      <p>
        One of the most striking example of Self-organization is the formation of multicellular organisms, where each cell can only interact with its local environment through its cell membrane, but based on the signals it receives from its neighbors and its own internal state, it is able to understand what type of cell it should be and self-regulate. Despite the limited information available to each individual cell, incredibly complex organisms can be formed.
      </p>
      <p>
        In this work we show how the same process of self-organization that occurs in biological systems can be used to create cohesive narrative skills in linguistic space, where the words in a text self-organize to create a coherent whole.
      </p>
      <p>
        But what makes a system capable of self-organizing in the first place? In this paper we show that the ability of a system to self-organize relies on having the right topology for cell communication. Furthermore, using tools from Statistical Physics, we show that current state of the art architectures for text generation lack the appropriate topology to be able to generate arbitrarily long, coherent text.
      </p>
    </div>
    <div target_audience="p" style="display: none;">
      <d-section id="abs">Abstract for Physicists</d-section>

      <p>
        One of the things that makes phase transitions beautiful is their universality. Despite the vast variety and complexity of systems of many particles, there are just a few of different phases of aggregation, and phase transitions are described by just a handful of critical exponents.
      </p>
      <p>
        Furthermore, we know that at low temperature lots of systems reach an ordered state, for example the ferromagnetic phase in Ising models where all the spins are all aligned, and the solid phase in materials where a crystalline structure is formed.
      </p>
      <p>
        In this paper we are going to show that asking if it is possible to generate arbitrarily long, coherent text is equivalent to asking if the system is capable of having an ordered phase.
      </p>
      <p>
        But what makes a system capable of self-organizing in the first place? In this paper we show that the ability of a system to self-organize relies on having the right topology for cell communication. Furthermore, we show that current state of the art architectures for text generation lack the appropriate topology to be able to generate arbitrarily long, coherent text.
      </p>
    </div>

    <div target_audience="m" style="display: none;">
      <d-section id="abs">Abstract for ML researchers</d-section>
      <p>
      The complexity of generating a text with $N$ words with modern machine learning algorithms such as the Transformer is $O(N^2)$. This makes it computationally infeasible to generate coherent pieces of text as long as books. Self-organizing systems, on the other hand, have two big strenghts that transformers lack
      </p>
      <p>
        <ul>
          <li>
            Efficency: Self-organizing systems often exhibit efficient resource allocation and optimization of processes. The emergent patterns and structures that arise from local interactions can lead to efficient distribution of tasks, information, or resources, resulting in overall system efficiency
          </li>
          <li>
            Scalability: Self-organizing systems can often scale effectively. As the number of components increases the system can still organize itself without requiring additional centralized control. This property is valuable in various domains, such as computer networks, where scalability is crucial
          </li>
        </ul>
      </p>
      <p>Inspired from Biology and using tools from Statistical Physics we show that:</p>
      <p>
        <ul>
          <li>
            Autoregressive models, no matter the architecture, the number of parameters, the window size, and the training procedure will never be able to generate text that is coherent at arbitrarily long distances.
          </li>
          <li>
            Transformers with attention matrices with a specifically crafted topology, can in principle be able to generate text that is coherent at arbitrarily long distances.
          </li>
        </ul>
      </p>
    </div>
    


















    <div target_audience="b m">
    <d-section id="intro">The simplest example of self-organization: The Ising Model</d-section>
    <p>
      Initially, when I delved into the study of the Ising model, I found it utterly useless and couldn't comprehend why physicists were so fixated on this seemingly mundane model. Consequently, I mustered the bare minimum effort required to pass my exams, and as expected, my brain discarded the information it deemed irrelevant, causing me to completely forget about it.<br><br>

      Upon graduating and finding myself officially unemployed, I had an abundance of time on my hands. Curiously, I found myself engrossed in studying more than I ever did during my university days. A few months prior to graduating, I had even published a piece on self-organization in Cellular Automata<d-cite key="cavuoti2022adversarial"></d-cite>. However, the results didn't seem general enought for me<d-footnote>Most of the results in Machine learning are like this, you find an algorithm that works, and then you say something handwavy that tries to explain wat is going on</d-footnote> and, therefore, turned back to the realm of statistical physics in search of answers.
      <blockquote>
        I now understood that in order to understand Self-Organizing systems in general, I had to understand the simplest of them all: The Ising model.
      </blockquote>
    </p>
    <p>
      But what exactly is the Ising model? Originally conceived as a simple explanation for magnetization effects in matter, the Ising model explores the interplay between interacting spins. These spins, when in proximity, have a preference to align with each other, leading to an interaction energy denoted by $H=-Js_1\cdot s_2$.<br><br>

      But what happens when lots of spins interact? Whell, it depends on how they interact: The Ising Model comes in many different flavors, we are going to look at the 3 simplest ones: the fully connected, the 1D and the 2D one.
    </p>

    <d-subsection>Fully connected Ising model</d-subsection>
    <p>
      In the fully connected Ising model we have L spins, all interacting with each other. The Hamiltonian of the system is 

      $$H=-\frac JN\sum_{i\neq j}s_is_j$$

      Now lets see how the system behaves as a function of the temperature.
    </p>
    <figure>
      <canvas id="FC_ising-canvas" width="700" height="700"></canvas>
    <div>
        <label for="temperature-slider">Temperature:</label>
        <input type="range" id="FC_temperature-slider" min="0.1" max="5.0" step=".01" value="1.0">
        <span id="FC_temperature-value">1.0</span>
    </div>
    
      <figcaption>
        Simulation of the Fully connected Ising Model
      </figcaption>
    </figure>
    <p>
      As you can see, below a certain <i>Critical Temperature</i> $T_\textrm c$ you have an <i>Ordered Phase</i>, while above you have a <i>Disordered Phase</i>
    </p>

    <d-subsection>1D Ising Model</d-subsection>
      <p>
      In the 1D Ising Model, spins are arranged in a chain and interact with their neighboring spins. The system's Hamiltonian can be expressed as:
      $$
      H=-J\sum_{i}s_is_{i+1}
      $$
      Now, let's examine the behavior of this system in relation to temperature.
      </p>
      <figure>
        <canvas id="canvas1D" width="700" height="20" style="border: 1px solid #000;"></canvas>
        <div>
            <label for="temperature">Temperature:</label>
            <input type="range" id="temperature" min="0.1" max="3" step="0.1" value="1">
        </div>
        
      <figcaption>
        Simulation of the 1D Ising Model (TODO)
      </figcaption>
      </figure>
      <p>
      As observed, regardless of how low the temperature is, the system never manages to achieve an ordered phase. This can be proven mathematically with the following theorem
      </p>
      <h3 class="theorem">Theorem</h3>
      <p>
        The 1D Ising chain doesn't have an ordered phase
      </p>
      <details>
        <summary>
          <b>proof</b>
        </summary>
        <p>
          At thermal equilibrium, the Free Energy $F$ is minimized.<d-footnote>This is because the Free Energy measures the amount of useful energy that can be extracted from a thermodynamic system. At thermal equilibrium, no useful energy can be extracted.</d-footnote></p>
        <p>
          To demonstrate that the ordered state, where all spins align, is not the minimum of Free Energy, we have to examine what occurs when a domain wall forms.
        </p>
        <figure>
          <img src="website/images/domain_wall.png" style="width:100%; height:auto; float:left;">
          <figcaption>
            The top image shows the ordered phase of the Ising model, while the bottom image depicts a state with a domain wall.
          </figcaption>
        </figure>
        <p>
          Upon the formation of a domain wall, the energy change increases by $2J$. This is because only a single link now comprises two opposite spins.<br><br>
          If the chain is $L$ spins long, there can be $L-1$ possible locations for the domain wall. Consequently, the size of the configuration space where a domain wall is present, denoted as $\Omega$, equals $\Omega = L-1$. Since entropy is the logarithm of the configuration space, we have $\Delta S = \log(L-1)$.<br><br>
          Therefore, the change in Free Energy upon the formation of a domain wall is given by:
          $$
          \Delta F= \Delta E -T\Delta S= J - T\log(L-1)
          $$
          In the thermodynamic limit, as $L$ approaches infinity, and since $J$ is a constant, we find that for any temperature $T$, $\Delta F < 0$.<br><br>
          Thus no matter how low the temperature (or the noise) is, it is impossible to reach an ordered state
        </p>
      </details>
      
    <d-subsection>2D Ising model</d-subsection>
    <p>
      In the 2D Ising model the spins are arranged in a grid, and each one of them interacts with his neighbours. The system's Hamiltonian can be expressed as
      $$
        H=-J\sum_{\langle i,j\rangle} s_is_j
      $$
      Where $\langle i, j\rangle$ means that we sum over all the neighboring spins. And let's see how it behaves
      
    </p>
    <canvas id="canvas2D" width="700" height="700" style="border: 1px solid #000;"></canvas>
    <div>
        <input type="range" id="temperature2D" min="0.1" max="3" step="0.1" value="1">
        <label for="temperature2D">$T=$</label>
    </div>
    <figcaption>
      Simulation of the 2D ising model
    </figcaption>
    
    <p>
      As you can see, when the temperature is sufficiently low, the system will slowly reach an ordered phase. 
    </p>
    <h3 class="theorem">Theorem</h3>
    <p>The 2D Ising model does have an ordered phase </p>
      <details>
        <summary>
          <b>proof</b>
        </summary>
        <p>The following proof is also known as <i>Peierls Argument</i> <d-cite key="bonati2014peierls, peierls1936ising"></d-cite>.</p>
        <p>
          Suppose we start in a configuration where all the spins are all in the $+1$ configuration, we now create a domain with a perimeter $\mathcal P$ made of spins in the $-1$ configuration, and see what happens to the Free Energy.
        </p>
        <p>
          The change in energy will be $$\Delta E= 2J\times \mathcal P$$ 
          Where $J$ is the coupling strenght between spins
        </p>
        <p>
          The change in Entropy is harder to estimate, fortunately we only need an upper bound.<br>
          We need to find the number of domain with a perimeter $\mathcal P$.
        </p>
        <p>
          We can always fit a curve of perimeter $\mathcal P$ inside a box $\mathcal P\times \mathcal P$ centered in the origin, so the number of starting points is at most $\mathcal P^2$, then at each step along the perimeter the curve has at most 3 choices for where to step next, so for each starting point there are at most $3^{\mathcal P}$ curves <d-footnote>The choices are usually less than 3 beacuse the curve is a close self-avoiding curve, so to satisfy this constraints usually has less choices. It can be proven that there are at least $2^{\mathcal P/2}$ possible configurations FIND THE SOURCE</d-footnote>
        </p>
        <p>
          We can thus conclude that the number of possible configuration $\Omega\le \mathcal P^23^\mathcal P$, and the change in entropy is
          $$
            \Delta S\le \mathcal P\log3 +2\log\mathcal P
          $$
          Putting it all toghether we have that
          $$\Delta F \ge 2J\mathcal P - T \mathcal P\log3 +O(\log \mathcal P)$$
          And if $T$ is sufficently low we have that $\Delta F>0$, thus for low temperature the 2D Ising model at the equilibrium tends to an ordered phase.
        </p>

      </details>
      <p>
        However, there is a caveaut here: The theorem doesn't tell us anything about how much time the system takes to reach equilibrium, infact if the system is sufficently large it can take more then the age of the universe.
      </p>
    <p>
      If you want some greate resources for learning about the Ising model and phase transitions in general I suggest you to look at this books <d-cite key="tong2017statistical, kardar2007statistical"></d-cite>
    </p>
    </div>


    <d-section>Associative Memory</d-section>
    <p>
      The brain is capable of storing and retreiving memories, but the way it does that is very different on how the memories are managed in a computer.
      <ul>
        <li>
          <b>Address based memories</b>, which are the kind of memories stored in a computer, work that given a memory address, it gives you back the information stored in that specific address.
        </li>
        <li>
          <b>Associative memories</b> are how memories are stored and extracted in human brains. In an associative memory system, a memory becomes easier to extract the more “hints” or related memories are given.
        </li>
      </ul>
    </p><p>
      So basically, in an associative memory, when you give him a prompt, it gives you the most associated memory stored to the prompt given<d-footnote>You might already suspect where this is going, but for now we will stay away from Natural Language Processing</d-footnote><d-footnote>One such example in computer science is what happens when you Google something, it gives you a list of related links stored somewhere on the internet.</d-footnote>.
    </p>
    <p>
      In general, a associative memory can be modeled as a energy landscape with with a discrete number of minima, each of which corresponding to a stored memory
      <d-figure>
        <img src="website/images/associative_memory-1.png" width="100%">
        <figcaption>
          Example of associative memory, in this case each member of the Simpsons family has his own enegry minimum. When given a guess of the system will converge to the closest Simpsons member.
        </figcaption>
      </d-figure>
    </p>
    <p>
      Associative memories, just like the merory inside a computer have finite capacity, but how do you quintify this?<br>
      Intuitively what ends up happening when you try to store too many separate memories, the configuration space becomes too crammed and some of the saved patterns fuse toghered.
      <d-figure>
        <img src="website/images/associative_memory-2.png" width="100%">
        <figcaption>
          Example of a saturated associative memory, when lots of patters are stored, minima that are close togheder fuse and are no longer distinguishable.
        </figcaption>
      </d-figure>
    </p>
    
    <d-subsection>Morphogenesis and Language Modelling</d-subsection>
    <p>
      Other self-organizing systems like mophogenesis can also be seen though the lens associative memory.
      WORK IN PROGRESS
    </p>
    <d-subsection>Hopfield Networks</d-subsection>
    <p>
      Hopfield Networks <d-cite key="hopfield1982neural"></d-cite> are one of the most famous examples of associative memories, and, it turns out that, when generalized, they are equivalent to the Transformer architecture <d-cite key="ramsauer2020hopfield"></d-cite>. It is the perfect working bench to understand what are the capabilities of associative memeories.
    </p>
    <p>
      The standard Hopfield Network with $N$ patterns stored equation is the following
      $$
        H=\sum_{ij}W_{ij}s_is_j\quad\textrm{ with }\quad W_{ij}=\sum_p^NX^p_{i}X^p_{j}
      $$
      Notice how this is similar to the equation of the Hamiltonian of the fully connected Ising model <d-footnote>...Actually if there is just one stored pattern they are equivalent up to a Gauge transformation $s_i\to X_is_i$</d-footnote>. What effectively does is that it creates a number of local minima equal to the number of patterns stored, here is a simulation.
      <d-figure>
        <canvas id="canvasHop" width="600" height="600"></canvas>
        <div>
            <label for="temperatureHop-slider">Temperature:</label>
            <input type="range" id="temperatureHop-slider" min="0.1" max="3.0" step=".01" value="1.0">
            <span id="temperatureHop-value">1.0</span>
        </div>

    
        <figcaption>
          Here is a simulation of the Hopfield model, the two patterns stored are a image of Luffy and Zoro from One piece. ADD REFERENCE IMAGES<br>
          You can go from a saved pattern to the other by increasing the temperature and relowering (it works 50% of the times). If you dont want to wait for the system to thermalize you can just reload the page.
        </figcaption>
      </d-figure>
    </p>
    <p>
      As you can see at low temperature the system converges to the closest state, but at hight temperature the system is noisy.<br><br>
      <br>The statistics of this system has been extensively studied in the literature <d-cite key="amit1987statistical, mezard2017mean"></d-cite>, and it has been shown that the total number of patterns $N$ that it is possible to store  in this kind of memory is $N\approx 0.14\times\textrm{number of pixels}$<d-footnote>This estimate assumes that the patterns are statistically indipendent. For real life data this is usually not true as the patterns are hardly ever statistically indipendent, so it should be taken as an upper bound</d-footnote>.
    </p>
    <p>
      This memory capacity is pretty low, but thankfully it is possible to increase it by changing the Hamiltonian in this way.
      $$
        H=-\sum_p^N F\left(\sum_i X^p_is_i \right)
      $$
      We know that if $F(x)\approx x^n$ then the storage capacity increases as $\alpha_n L^{n-1}$ <d-cite key="krotov2016dense"></d-cite>, and if $F(x)\approx e^x$ the the storage capacity goes like $e^{\alpha L}$ <d-cite key="demircigil2017model"></d-cite> SHOULD I EXPLAIN MORE HERE? MAYBE WITH A SPOILER?
    </p>
  
  <d-subsection>Hopfield networks with local interaction</d-subsection>
    <p>
      Many of the self-organizing systems in real life are capable to reach an highly ordered state even with just local interactions, so what happens if the Hamiltonian of the Hopfield network makes spins interact only if they are close by?
      $$
        H=\sum_{\langle i,j\rangle}W_{ij}s_is_j\quad\textrm{ with }\quad W_{ij}=\sum_p^NX^p_{i}X^p_{j}
      $$
      In this case $\langle i,j\rangle$ means that $i$ and $j$ are in the same window of interaction. Here is a simutation to play with.
    </p>
    <d-figure>
      <canvas id="canvasHop2D" width="600" height="600"></canvas>
        <div>
            <label for="temperatureHop2D-slider">Temperature:</label>
            <input type="range" id="temperatureHop2D-slider" min="0.01" max="5.0" step=".1" value="1.0">
            <span id="temperatureHop2D-value">1.0</span>
        </div>
    </d-figure>
    <p>
      As you can see the system strugles more to converge to one of the saved patters, and the smaller the window size is, the harder it becomes.<br> What it is possible to conclude even with this small demo is that the memory capacity of a local Hopfield Model is lower, we will later analize by how much.
    </p>
  <d-subsection>Transformers and Hopfield networks</d-subsection>
    Maybe its best to put this subsection later
<d-section>
  What makes a self-organizing system capable of organizing?
</d-section>
  <p>
    So far we have seen that
    <ul>
      <li>The 1D Ising model is not capable of self-organizing in to an ordered phase</li>
      <li>The 2D Ising model is capable of self-organizing in to an ordered phase, but it takes time</li>
      <li>The Fully connected Ising model is capable of self-organizing in to an ordered phase and it does that quickly</li>
      <li>The Standard Hopfield Network, which is a generalization of the Fully connected Ising model is capable of self-organizing quicky, but it doesn't have a large memory capacity</li>
      <li>Windowed Hopfield wich are a generalization of the 2D Ising model are in principle capable of self organizing, but their memory capacity is lower than the one of standard Hopfield Networks, and it takes time to self-organize</li>
    </ul>
    But is there a unified way of determining if a system is capable of self organizing?
  </p>
  <p>
    It turns out that there is, but before proving in the general case, we will first prove why autoregressive models for text are incapable of generating text that remains coherent at arbitrary lenghts as it gives us the intuition to prove it in the more general case
  </p>


  </d-article>
  <d-bibliography src="website/bibliography.bib"></d-bibliography>
  <script src="website/scripts/fully_connected_ising.js"></script>
  <script src="website/scripts/1Dising.js"></script>
  <script src="website/scripts/2Dising.js"></script>
  <script src="website/scripts/standard_hopfield.js"></script>
  <script src="website/scripts/2Dhopfield.js"></script>
  <script rel="text/javascript" src="website/sourcecode.js"> </script>

</body>

<!--
    <p>
    As you can see from equation <d-reference key="gauss"></d-reference>  and section <d-reference key="remarks"></d-reference> the phase of the wavefunction is not observable. <d-cite key="shneiderman1996eyes"></d-cite>
</p>
-->