{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch, math, einops\n",
    "from src import Tokenizer, linear_unidirectional_graph_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('gpt2')\n",
    "pretraied = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.GPT2 import GPT2, GPT2_Encoder, GPT2_LM_Head\n",
    "\n",
    "encoder=GPT2_Encoder()\n",
    "decoder=GPT2_LM_Head()\n",
    "\n",
    "model=GPT2(encoder, decoder, tokenizer)\n",
    "\n",
    "model.load_from_original(pretraied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randint(0, 50257, (1, 10))\n",
    "x=torch.randn([1,13,768])\n",
    "\n",
    "sequence_lenght=17\n",
    "batch_size=1\n",
    "n_heads=13\n",
    "d_Embedding=64*n_heads\n",
    "Q=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "K=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "V=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "\n",
    "\n",
    "out, att = pretraied.transformer.h[0].attn._attn(Q,K,V,None,None)\n",
    "#out=out.permute(1, 0, 2).contiguous().view(batch_size, sequence_lenght, d_Embedding)\n",
    "out.shape,att.shape\n",
    "out=out.permute(0,2,1,3).view(sequence_lenght,n_heads,d_Embedding//n_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_pretrained = einops.einsum(Q,K,'... s e, ... t e -> ... s t')/math.sqrt(K.size(-1))\n",
    "alignments=att_pretrained\n",
    "query_length, key_length = Q.size(-2), K.size(-2)\n",
    "\n",
    "causal_mask = pretraied.transformer.h[0].attn.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n",
    "mask_value = -torch.tensor(float('inf'))\n",
    "att_pretrained = torch.where(causal_mask, att_pretrained, mask_value)\n",
    "alignments=att_pretrained\n",
    "att_pretrained = torch.nn.functional.softmax(att_pretrained, dim=-1)\n",
    "att_pretrained = att_pretrained.view(n_heads,sequence_lenght,sequence_lenght)\n",
    "\n",
    "(att_pretrained==att).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3968, -1.3849,  1.6391, -0.3263, -2.3599,  0.2857,  0.6945, -0.7083,\n",
      "         1.2523,  0.9548, -0.9761,  0.3153, -1.8931])\n",
      "tensor([-0.3968, -1.3849,  1.6391, -0.3263, -2.3599,  0.2857,  0.6945, -0.7083,\n",
      "         1.2523,  0.9548, -0.9761,  0.3153, -1.8931])\n"
     ]
    }
   ],
   "source": [
    "from src.transformerMP import attention_message\n",
    "\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "\n",
    "edge_index=graph_maker(sequence_lenght)\n",
    "senders, receivers = edge_index\n",
    "\n",
    "Q1=Q.view(Q.shape[1:]).permute(1,0,2)\n",
    "K1=K.view(K.shape[1:]).permute(1,0,2)\n",
    "V1=V.view(V.shape[1:]).permute(1,0,2)\n",
    "\n",
    "#o,a=attention_message(Q,K,V,edge_index,0)\n",
    "#o.view(sequence_lenght,d_Embedding)\n",
    "\n",
    "N=20\n",
    "\n",
    "i,j=receivers[N],senders[N]\n",
    "print((Q1[receivers]*K1[senders]).sum(dim=-1)[N]/math.sqrt(K.size(-1)))\n",
    "\n",
    "print(alignments.view(n_heads,sequence_lenght,sequence_lenght)[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformerMP import normalize_strength\n",
    "from math import sqrt\n",
    "def attention_message(Q:torch.Tensor,\n",
    "                      K:torch.Tensor,\n",
    "                      V:torch.Tensor,\n",
    "                      edge_index:torch.Tensor,\n",
    "                      att_dropout=0.1\n",
    "                      ):\n",
    "\n",
    "    senders, receivers = edge_index\n",
    "    N,h,d=K.shape   \n",
    "\n",
    "    #Q.K^T \n",
    "    att=(Q[receivers]*K[senders]).sum(dim=-1)/sqrt(d)\n",
    "\n",
    "    #softmax    \n",
    "    att = torch.exp(att+3-att.max()) #could be done in-plase using the function att.exp_() if memory is a bootleneck\n",
    "    attention = normalize_strength(att, receivers, N, h)\n",
    "\n",
    "    #Dropout\n",
    "    #att=attention_dropout(attention, att_dropout)\n",
    "\n",
    "    #softmax*V\n",
    "    att = einops.einsum(att,V[senders],' ... , ... c -> ... c')\n",
    "    out=torch.zeros_like(V,device=V.device)\n",
    "\n",
    "    return out.index_add(0,receivers,att), attention #could be done in-place using the function out.index_add_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5],\n",
      "        [5, 5, 5, 5, 5, 5]])\n",
      "tensor([0.0351, 0.0388, 0.1542, 0.1554, 0.0691, 0.3066, 0.0038, 0.0071, 0.1523,\n",
      "        0.0427, 0.0638, 0.0581])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0351, 0.0388, 0.1542, 0.1554, 0.0691, 0.3066, 0.0038, 0.0071, 0.1523,\n",
       "        0.0427, 0.0638, 0.0581])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o,a=attention_message(Q1,K1,V1,edge_index,0)\n",
    "\n",
    "i=3\n",
    "print(edge_index[:,receivers==j])\n",
    "print(att_pretrained[i,j:,j])\n",
    "a[senders==j,i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([153, 13]) torch.Size([13, 17, 17]) torch.Size([1, 13, 17, 17])\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(a.shape,att_pretrained.shape,att.shape)\n",
    "\n",
    "for j in range(att_pretrained.shape[-1]):\n",
    "    print(\n",
    "        torch.allclose(att_pretrained[:,j:,j],a.permute(1,0)[:,senders==j],1e-4,1e-4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 13, 64]) torch.Size([17, 13, 64])\n",
      "tensor([-0.6116,  1.0904,  0.5852, -0.4475, -2.0778,  1.7976,  0.3143,  2.6921,\n",
      "        -1.3339,  1.5672, -0.3637, -1.6769,  0.4228])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.4258,  0.0827,  0.1115, -0.1655, -1.5706,  0.8585,  0.0904,  1.5680,\n",
       "        -1.0546,  0.1221, -0.2285, -1.0437,  0.1066])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out.shape,o.shape)\n",
    "print(out[0,:,0])\n",
    "o[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.8232, 0.7116, 0.5459, 0.6313, 0.3193, 0.4593, 0.5120, 0.5217, 0.2086,\n",
       "         0.8486, 0.4482, 0.3552, 0.4378]),\n",
       " tensor([0.6555, 0.4965, 0.8754, 0.8949, 0.2480, 0.5895, 0.3286, 0.0593, 0.2792,\n",
       "         0.5256, 0.2686, 0.2887, 0.3042]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a.shape,att.shape\n",
    "N=1\n",
    "\n",
    "i,j=senders[N],receivers[N]\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "a[N],att_pretrained[:,j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n",
      "tensor([0.3432, 0.5988, 0.2493, 0.1185, 0.2133, 0.1271, 0.5732, 0.2741, 0.4261,\n",
      "        0.5167, 0.0223, 0.0487])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4503, 0.0660, 0.3172, 0.8236, 0.3810, 0.2807, 0.1164, 0.0755, 0.0994,\n",
       "        0.2715, 0.6308, 0.7693, 0.5859])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "\n",
    "print(edge_index[:,i])\n",
    "attn_weights=att_pretrained.view(n_heads,sequence_lenght,sequence_lenght)\n",
    "print(a[i])\n",
    "attn_weights[:,receivers[i],senders[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_2889/3492215469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "torch.isclose(o,out,1e-3,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0409,  1.3881,  4.1935,  ..., -0.0925,  0.1177,  0.0000],\n",
       "        [ 1.0567, -1.3801,  1.3112,  ..., -0.0530, -0.0237,  0.1608],\n",
       "        [-4.0899,  1.3798,  1.4433,  ...,  0.0746,  0.0000,  0.3461],\n",
       "        ...,\n",
       "        [ 0.1670, -0.1714, -0.1629,  ...,  0.1357, -0.0147,  0.0000],\n",
       "        [ 0.1670, -0.1714, -0.1629,  ...,  0.1357, -0.0147,  0.0315],\n",
       "        [ 2.9507,  3.0202,  0.3895,  ...,  0.1534,  0.1628,  0.0112]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x.view(x.shape[1:])\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "edge_index=graph_maker(x.shape[0])\n",
    "\n",
    "model.transformer_blocks[0].attention_block(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import linear_unidirectional_graph_maker\n",
    "\n",
    "sample_text = \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry\"\n",
    "x=tokenizer(sample_text)\n",
    "\n",
    "graph_maker=linear_unidirectional_graph_maker(20)\n",
    "edge_index=graph_maker(len(x))\n",
    "#x,edge_index\n",
    "tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.5262, 0.9352, 0.6346, 0.9176, 0.9545, 0.4873, 0.6251, 0.3462, 0.5225,\n",
       "        0.6778, 0.4525, 0.8533, 0.7261, 0.6538, 0.7236, 0.5359, 0.4798, 0.5364],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([  262,   262, 47490,   262,   262, 47490,   262,   262,   262,   262,\n",
       "        47490,   262,   262,   262, 47490,   262, 47490,   262]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=model(x,edge_index)\n",
    "out.max(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'��� the the��� the the indo the the the the confir the the the the, the'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "288\n",
      "torch.Size([32, 8, 1])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of parameters in a model.\"\"\"\n",
    "    return sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "conv = torch.nn.Conv1d(8,32,1)\n",
    "print(count_parameters(conv))\n",
    "# 288\n",
    "\n",
    "linear = torch.nn.Linear(8,32)\n",
    "print(count_parameters(linear))\n",
    "# 288\n",
    "\n",
    "print(conv.weight.shape)\n",
    "print(conv.bias.shape)\n",
    "# torch.Size([32, 8, 1])\n",
    "print(linear.weight.shape)\n",
    "print(linear.bias.shape)\n",
    "# torch.Size([32, 8])\n",
    "\n",
    "# use same initialization\n",
    "linear.weight = torch.nn.Parameter(conv.weight.squeeze(2))\n",
    "linear.bias = torch.nn.Parameter(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94cac80b40987d9dfcd7d9664f83c8b2cf119f0030fe115bcc4b81bcd6645dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
