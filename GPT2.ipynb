{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch, math, einops\n",
    "from src import Tokenizer, linear_unidirectional_graph_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('gpt2')\n",
    "pretraied = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretraied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Model' object has no attribute 'lm_f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_22967/1573025344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretraied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Language_CA/src/GPT2.py\u001b[0m in \u001b[0;36mload_from_original\u001b[0;34m(self, pretrained_model)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Load the language model head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1270\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Model' object has no attribute 'lm_f'"
     ]
    }
   ],
   "source": [
    "from src.GPT2 import GPT2, GPT2_Encoder, GPT2_LM_Head\n",
    "\n",
    "encoder=GPT2_Encoder()\n",
    "decoder=GPT2_LM_Head()\n",
    "\n",
    "model=GPT2(encoder, decoder, tokenizer,dropout=0)\n",
    "\n",
    "model.load_from_original(pretraied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randint(0, 50257, (1, 10))\n",
    "x=torch.randn([1,13,768])\n",
    "\n",
    "sequence_lenght=17\n",
    "batch_size=1\n",
    "n_heads=13\n",
    "d_Embedding=64*n_heads\n",
    "Q=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "K=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "V=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "\n",
    "\n",
    "out, att = pretraied.transformer.h[0].attn._attn(Q,K,V,None,None)\n",
    "#out=out.permute(1, 0, 2).contiguous().view(batch_size, sequence_lenght, d_Embedding)\n",
    "out.shape,att.shape\n",
    "out=out.permute(0,2,1,3).view(sequence_lenght,n_heads,d_Embedding//n_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_pretrained = einops.einsum(Q,K,'... s e, ... t e -> ... s t')/math.sqrt(K.size(-1))\n",
    "alignments=att_pretrained\n",
    "query_length, key_length = Q.size(-2), K.size(-2)\n",
    "\n",
    "causal_mask = pretraied.transformer.h[0].attn.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n",
    "mask_value = -torch.tensor(float('inf'))\n",
    "att_pretrained = torch.where(causal_mask, att_pretrained, mask_value)\n",
    "alignments=att_pretrained\n",
    "att_pretrained = torch.nn.functional.softmax(att_pretrained, dim=-1)\n",
    "att_pretrained = att_pretrained.view(n_heads,sequence_lenght,sequence_lenght)\n",
    "\n",
    "(att_pretrained==att).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9323,  1.3134,  0.0814, -0.7573,  1.5721,  1.6462,  0.7974, -0.1905,\n",
      "        -0.8649, -1.7122, -0.2299,  0.9718, -0.9527])\n",
      "tensor([ 0.9323,  1.3134,  0.0814, -0.7573,  1.5721,  1.6462,  0.7974, -0.1905,\n",
      "        -0.8649, -1.7122, -0.2299,  0.9718, -0.9527])\n"
     ]
    }
   ],
   "source": [
    "from src.transformerMP import attention_message\n",
    "\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "\n",
    "edge_index=graph_maker(sequence_lenght)\n",
    "senders, receivers = edge_index\n",
    "\n",
    "Q1=Q.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "K1=K.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "V1=V.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "\n",
    "#o,a=attention_message(Q,K,V,edge_index,0)\n",
    "#o.view(sequence_lenght,d_Embedding)\n",
    "\n",
    "N=20\n",
    "\n",
    "i,j=receivers[N],senders[N]\n",
    "print((Q1[receivers]*K1[senders]).sum(dim=-1)[N]/math.sqrt(K.size(-1)))\n",
    "\n",
    "print(alignments.view(n_heads,sequence_lenght,sequence_lenght)[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformerMP import normalize_strength\n",
    "from math import sqrt\n",
    "def attention_message(Q:torch.Tensor,\n",
    "                      K:torch.Tensor,\n",
    "                      V:torch.Tensor,\n",
    "                      edge_index:torch.Tensor,\n",
    "                      att_dropout=0.1\n",
    "                      ):\n",
    "\n",
    "    senders, receivers = edge_index\n",
    "    N,h,d=K.shape   \n",
    "\n",
    "    #Q.K^T \n",
    "    att=(Q[receivers]*K[senders]).sum(dim=-1)/sqrt(d)\n",
    "\n",
    "    #softmax    \n",
    "    att = torch.exp(att+3-att.max()) #could be done in-plase using the function att.exp_() if memory is a bootleneck\n",
    "    attention = normalize_strength(att, receivers, N, h)\n",
    "\n",
    "    #Dropout\n",
    "    #att=attention_dropout(attention, att_dropout)\n",
    "\n",
    "    #softmax*V\n",
    "    att = einops.einsum(attention,V[senders],' ... , ... c -> ... c')\n",
    "    out = torch.zeros_like(V,device=V.device)\n",
    "\n",
    "    return out.index_add(0,receivers,att), attention #could be done in-place using the function out.index_add_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5],\n",
      "        [5, 5, 5, 5, 5, 5]])\n",
      "tensor([0.0936, 0.1358, 0.0410, 0.2424, 0.2716, 0.0526, 0.2126, 0.0540, 0.1150,\n",
      "        0.0570, 0.0814, 0.0564])\n",
      "tensor([0.0936, 0.1358, 0.0410, 0.2424, 0.2716, 0.0526, 0.2126, 0.0540, 0.1150,\n",
      "        0.0570, 0.0814, 0.0564])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_function,a=attention_message(Q1,K1,V1,edge_index,0)\n",
    "\n",
    "i=3\n",
    "print(edge_index[:,receivers==j])\n",
    "print(att_pretrained[i,j:,j])\n",
    "print(a[senders==j,i])\n",
    "asd=True\n",
    "for j in range(att_pretrained.shape[-1]):\n",
    "    asd= asd and torch.allclose(att_pretrained[:,j:,j],a.permute(1,0)[:,senders==j],1e-4,1e-4)\n",
    "asd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 17, 17]) torch.Size([13, 17, 64])\n",
      "torch.Size([13, 17, 17, 64])\n",
      "torch.Size([153, 13]) torch.Size([153, 13, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "V=V.view(n_heads,sequence_lenght,d_Embedding//n_heads)\n",
    "att_pretrained.shape,V.shape\n",
    "print(att_pretrained.shape,V.shape)\n",
    "AV_pretrained=einops.einsum(att_pretrained,V,'h i j, h j e -> h i j e')\n",
    "print(AV_pretrained.shape)\n",
    "\n",
    "if not (AV_pretrained.sum(-2)==torch.matmul(att_pretrained,V)).all(): #true\n",
    "    print (False)\n",
    "\n",
    "print(a.shape,V1[senders].shape)\n",
    "dsa=einops.einsum(a,V1[senders],'..., ... e -> ... e')\n",
    "dsa.shape==V1[senders].shape==(152, n_heads, d_Embedding//n_heads) #True\n",
    "\n",
    "N=9\n",
    "i,j=senders[N],receivers[N]\n",
    "print(torch.allclose(dsa[N],AV_pretrained[:,j,i],1e-3,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 13, 64]) torch.Size([17, 13, 64])\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pretrained=AV_pretrained.sum(-2).permute(1,0,2)\n",
    "dsum=torch.zeros_like(V1).index_add(0,receivers,dsa)\n",
    "\n",
    "print(out_pretrained.shape,dsum.shape)\n",
    "print(torch.allclose(out_pretrained,dsum,1e-3,1e-3))\n",
    "torch.allclose(out,out_function,1e-3,1e-3)\n",
    "#print((dsa[N]==asd[:,j,i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aasdasda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=13\n",
    "heads=12\n",
    "d_Embedding=64*heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "def merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)\n",
    "\n",
    "def original_c_attn(x,i=0):\n",
    "        query, key, value = pretraied.transformer.h[i].attn.c_attn(x).split(d_Embedding, dim=2)\n",
    "\n",
    "        dim=d_Embedding//heads\n",
    "        query = split_heads(query, heads, dim)\n",
    "        key = split_heads(key, heads, dim)\n",
    "        value = split_heads(value, heads, dim)\n",
    "\n",
    "        return query,key,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=torch.randn([1,sequence_length,d_Embedding])\n",
    "\n",
    "edge_index=graph_maker(13)\n",
    "Qp,Kp,Vp=original_c_attn(x)\n",
    "Q,K,P=model.transformer_blocks[0].attention_block.make_QKV(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 13, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.5202,  2.1009,  5.4433, -4.5101,  3.7691,  3.1631,  8.9820, -3.0704,\n",
       "         -1.4016, -2.7364,  2.7083, -4.1566, 12.2631],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([ 2.5202,  2.1009,  5.4433, -4.5101,  3.7691,  3.1631,  8.9820, -3.0704,\n",
       "         -1.4016, -2.7364,  2.7083, -4.1566, 12.2631],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QQ=Qp.view([heads,sequence_length,d_Embedding//heads]).permute(1,0,2)\n",
    "\n",
    "QQ[:,0,0],Q[:,0,0]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sawrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "ln_1=nn.LayerNorm(d_Embedding,eps=1e-3)\n",
    "ln_2=nn.LayerNorm(d_Embedding,eps=1e-3)\n",
    "\n",
    "def forward(hidden_states):\n",
    "\n",
    "    residual = hidden_states  # residual\n",
    "\n",
    "    hidden_states = ln_1(hidden_states)  # normalization\n",
    "\n",
    "    attn_outputs = pretraied.transformer.h[0].attn(hidden_states)\n",
    "    attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "    outputs = attn_outputs[1:]\n",
    "    # residual connection\n",
    "    hidden_states = attn_output + residual  # attention\n",
    "\n",
    "\n",
    "\n",
    "    residual = hidden_states  # reisdual again\n",
    "\n",
    "    hidden_states = ln_2(hidden_states)  # normalization\n",
    "\n",
    "    feed_forward_hidden_states = pretraied.transformer.h[0].mlp(hidden_states)  # mlp\n",
    "    # residual connection\n",
    "    hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "    outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "    #in my model it only return hidden states\n",
    "    # hidden_states, present, (attentions, cross_attentions)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn((1,sequence_length,d_Embedding))\n",
    "out1=forward(x)\n",
    "out2=pretraied.transformer.h[0](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 11.0140,   1.3385,   0.3527,  ...,   0.4152,  -6.2672,  -1.3940],\n",
       "         [  6.7424, -11.9847,  16.5153,  ...,   1.1466,  -2.2571,  -1.7462],\n",
       "         [ 14.0308,   6.2891,   1.8980,  ...,   1.1527,   2.8099,   0.7151],\n",
       "         ...,\n",
       "         [  3.4113,  10.9683,   9.1666,  ...,  18.5762,   3.4282,  15.2770],\n",
       "         [ 16.7814,  -7.9987, -23.0774,  ...,   1.2276,  -1.3529,   8.3471],\n",
       "         [ 20.9909,  -1.9449,  -4.7618,  ...,   8.8635,  -3.1906,   4.2272]],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[  8.6352,   3.6732,  -4.3042,  ...,  17.0297,   5.4470,   0.7072],\n",
       "         [ 32.1582,  -6.3796,  -7.5093,  ...,  12.4453,  -4.0244,  -7.5346],\n",
       "         [ 31.5250,  -5.3637,  -8.9045,  ...,   9.0292,   0.0418,  -0.5986],\n",
       "         ...,\n",
       "         [  7.7772,   6.9585,  -0.8895,  ...,   8.9632,   8.2517,  26.1103],\n",
       "         [ 11.0655,  -7.6909, -18.9837,  ...,   0.9446,   4.0766,  19.9787],\n",
       "         [ 18.9379,   1.8972, -19.4432,  ...,   5.3744,  12.9888, -10.2606]],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1[0][0],out2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\",,\\n morning'm you guys\", 'ㅋ DW YORKettel Democracy YORK ACA')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(\"Hello world! good i saw you\")\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "\n",
    "edge_index=graph_maker(x.shape[0])\n",
    "tokenizer.decode(pretraied(x)[0].argmax(dim=-1)),tokenizer.decode(model(x,edge_index).argmax(dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94cac80b40987d9dfcd7d9664f83c8b2cf119f0030fe115bcc4b81bcd6645dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
