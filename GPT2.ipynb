{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch, math, einops\n",
    "from src import Tokenizer, linear_unidirectional_graph_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('gpt2')\n",
    "pretraied = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.GPT2 import GPT2, GPT2_Encoder, GPT2_LM_Head\n",
    "\n",
    "encoder=GPT2_Encoder()\n",
    "decoder=GPT2_LM_Head()\n",
    "\n",
    "model=GPT2(encoder, decoder, tokenizer)\n",
    "\n",
    "model.load_from_original(pretraied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randint(0, 50257, (1, 10))\n",
    "x=torch.randn([1,13,768])\n",
    "\n",
    "sequence_lenght=17\n",
    "batch_size=1\n",
    "n_heads=13\n",
    "d_Embedding=64*n_heads\n",
    "Q=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "K=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "V=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "\n",
    "\n",
    "out, att = pretraied.transformer.h[0].attn._attn(Q,K,V,None,None)\n",
    "#out=out.permute(1, 0, 2).contiguous().view(batch_size, sequence_lenght, d_Embedding)\n",
    "out.shape,att.shape\n",
    "out=out.permute(0,2,1,3).view(sequence_lenght,n_heads,d_Embedding//n_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_pretrained = einops.einsum(Q,K,'... s e, ... t e -> ... s t')/math.sqrt(K.size(-1))\n",
    "alignments=att_pretrained\n",
    "query_length, key_length = Q.size(-2), K.size(-2)\n",
    "\n",
    "causal_mask = pretraied.transformer.h[0].attn.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n",
    "mask_value = -torch.tensor(float('inf'))\n",
    "att_pretrained = torch.where(causal_mask, att_pretrained, mask_value)\n",
    "alignments=att_pretrained\n",
    "att_pretrained = torch.nn.functional.softmax(att_pretrained, dim=-1)\n",
    "att_pretrained = att_pretrained.view(n_heads,sequence_lenght,sequence_lenght)\n",
    "\n",
    "(att_pretrained==att).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7951,  0.0413, -0.2327, -0.5042, -0.1293, -0.5635,  0.1652, -0.6069,\n",
      "        -0.9542, -1.1916, -1.6909,  1.3947,  0.1134])\n",
      "tensor([-0.7951,  0.0413, -0.2327, -0.5042, -0.1293, -0.5635,  0.1652, -0.6069,\n",
      "        -0.9542, -1.1916, -1.6909,  1.3947,  0.1134])\n"
     ]
    }
   ],
   "source": [
    "from src.transformerMP import attention_message\n",
    "\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "\n",
    "edge_index=graph_maker(sequence_lenght)\n",
    "senders, receivers = edge_index\n",
    "\n",
    "Q1=Q.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "K1=K.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "V1=V.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "\n",
    "#o,a=attention_message(Q,K,V,edge_index,0)\n",
    "#o.view(sequence_lenght,d_Embedding)\n",
    "\n",
    "N=20\n",
    "\n",
    "i,j=receivers[N],senders[N]\n",
    "print((Q1[receivers]*K1[senders]).sum(dim=-1)[N]/math.sqrt(K.size(-1)))\n",
    "\n",
    "print(alignments.view(n_heads,sequence_lenght,sequence_lenght)[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformerMP import normalize_strength\n",
    "from math import sqrt\n",
    "def attention_message(Q:torch.Tensor,\n",
    "                      K:torch.Tensor,\n",
    "                      V:torch.Tensor,\n",
    "                      edge_index:torch.Tensor,\n",
    "                      att_dropout=0.1\n",
    "                      ):\n",
    "\n",
    "    senders, receivers = edge_index\n",
    "    N,h,d=K.shape   \n",
    "\n",
    "    #Q.K^T \n",
    "    att=(Q[receivers]*K[senders]).sum(dim=-1)/sqrt(d)\n",
    "\n",
    "    #softmax    \n",
    "    att = torch.exp(att+3-att.max()) #could be done in-plase using the function att.exp_() if memory is a bootleneck\n",
    "    attention = normalize_strength(att, receivers, N, h)\n",
    "\n",
    "    #Dropout\n",
    "    #att=attention_dropout(attention, att_dropout)\n",
    "\n",
    "    #softmax*V\n",
    "    att = einops.einsum(attention,V[senders],' ... , ... c -> ... c')\n",
    "    out = torch.zeros_like(V,device=V.device)\n",
    "\n",
    "    return out.index_add(0,receivers,att), attention #could be done in-place using the function out.index_add_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3],\n",
      "        [3, 3, 3, 3]])\n",
      "tensor([0.2588, 0.2823, 0.1009, 0.0054, 0.3157, 0.0963, 0.0137, 0.0299, 0.1241,\n",
      "        0.1702, 0.0130, 0.1355, 0.1829, 0.0291])\n",
      "tensor([0.2588, 0.2823, 0.1009, 0.0054, 0.3157, 0.0963, 0.0137, 0.0299, 0.1241,\n",
      "        0.1702, 0.0130, 0.1355, 0.1829, 0.0291])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_function,a=attention_message(Q1,K1,V1,edge_index,0)\n",
    "\n",
    "i=3\n",
    "print(edge_index[:,receivers==j])\n",
    "print(att_pretrained[i,j:,j])\n",
    "print(a[senders==j,i])\n",
    "asd=True\n",
    "for j in range(att_pretrained.shape[-1]):\n",
    "    asd= asd and torch.allclose(att_pretrained[:,j:,j],a.permute(1,0)[:,senders==j],1e-4,1e-4)\n",
    "asd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 17, 17]) torch.Size([13, 17, 64])\n",
      "torch.Size([13, 17, 17, 64])\n",
      "torch.Size([153, 13]) torch.Size([153, 13, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "V=V.view(n_heads,sequence_lenght,d_Embedding//n_heads)\n",
    "att_pretrained.shape,V.shape\n",
    "print(att_pretrained.shape,V.shape)\n",
    "AV_pretrained=einops.einsum(att_pretrained,V,'h i j, h j e -> h i j e')\n",
    "print(AV_pretrained.shape)\n",
    "\n",
    "if not (AV_pretrained.sum(-2)==torch.matmul(att_pretrained,V)).all(): #true\n",
    "    print (False)\n",
    "\n",
    "print(a.shape,V1[senders].shape)\n",
    "dsa=einops.einsum(a,V1[senders],'..., ... e -> ... e')\n",
    "dsa.shape==V1[senders].shape==(152, n_heads, d_Embedding//n_heads) #True\n",
    "\n",
    "N=9\n",
    "i,j=senders[N],receivers[N]\n",
    "print(torch.allclose(dsa[N],AV_pretrained[:,j,i],1e-3,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 13, 64]) torch.Size([17, 13, 64])\n",
      "True\n",
      "torch.Size([17, 13, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pretrained=AV_pretrained.sum(-2).permute(1,0,2)\n",
    "dsum=torch.zeros_like(V1).index_add(0,receivers,dsa)\n",
    "\n",
    "print(out_pretrained.shape,dsum.shape)\n",
    "print(torch.allclose(out_pretrained,dsum,1e-3,1e-3))\n",
    "out,_=\n",
    "torch.allclose(out,out_function,1e-3,1e-3)\n",
    "#print((dsa[N]==asd[:,j,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([153, 13]) torch.Size([1, 13, 17, 17])\n",
      "tensor(0) tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.5362, 0.6045, 0.3577, 0.1286, 0.2299, 0.4903, 0.2952, 0.7785, 0.0389,\n",
       "         0.3664, 0.3424, 0.8425, 0.5351]),\n",
       " tensor([0.5362, 0.6045, 0.3577, 0.1286, 0.2299, 0.4903, 0.2952, 0.7785, 0.0389,\n",
       "         0.3664, 0.3424, 0.8425, 0.5351]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(a.shape,att.shape)\n",
    "N=1\n",
    "\n",
    "i,j=senders[N],receivers[N]\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "a[N],att_pretrained[:,j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n",
      "tensor([0.3432, 0.5988, 0.2493, 0.1185, 0.2133, 0.1271, 0.5732, 0.2741, 0.4261,\n",
      "        0.5167, 0.0223, 0.0487])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4503, 0.0660, 0.3172, 0.8236, 0.3810, 0.2807, 0.1164, 0.0755, 0.0994,\n",
       "        0.2715, 0.6308, 0.7693, 0.5859])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "\n",
    "print(edge_index[:,i])\n",
    "attn_weights=att_pretrained.view(n_heads,sequence_lenght,sequence_lenght)\n",
    "print(a[i])\n",
    "attn_weights[:,receivers[i],senders[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_2889/3492215469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "torch.isclose(o,out,1e-3,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0409,  1.3881,  4.1935,  ..., -0.0925,  0.1177,  0.0000],\n",
       "        [ 1.0567, -1.3801,  1.3112,  ..., -0.0530, -0.0237,  0.1608],\n",
       "        [-4.0899,  1.3798,  1.4433,  ...,  0.0746,  0.0000,  0.3461],\n",
       "        ...,\n",
       "        [ 0.1670, -0.1714, -0.1629,  ...,  0.1357, -0.0147,  0.0000],\n",
       "        [ 0.1670, -0.1714, -0.1629,  ...,  0.1357, -0.0147,  0.0315],\n",
       "        [ 2.9507,  3.0202,  0.3895,  ...,  0.1534,  0.1628,  0.0112]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x.view(x.shape[1:])\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "edge_index=graph_maker(x.shape[0])\n",
    "\n",
    "model.transformer_blocks[0].attention_block(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import linear_unidirectional_graph_maker\n",
    "\n",
    "sample_text = \"Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry\"\n",
    "x=tokenizer(sample_text)\n",
    "\n",
    "graph_maker=linear_unidirectional_graph_maker(20)\n",
    "edge_index=graph_maker(len(x))\n",
    "#x,edge_index\n",
    "tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.5262, 0.9352, 0.6346, 0.9176, 0.9545, 0.4873, 0.6251, 0.3462, 0.5225,\n",
       "        0.6778, 0.4525, 0.8533, 0.7261, 0.6538, 0.7236, 0.5359, 0.4798, 0.5364],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([  262,   262, 47490,   262,   262, 47490,   262,   262,   262,   262,\n",
       "        47490,   262,   262,   262, 47490,   262, 47490,   262]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out=model(x,edge_index)\n",
    "out.max(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'��� the the��� the the indo the the the the confir the the the the, the'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "288\n",
      "torch.Size([32, 8, 1])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 8])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of parameters in a model.\"\"\"\n",
    "    return sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "conv = torch.nn.Conv1d(8,32,1)\n",
    "print(count_parameters(conv))\n",
    "# 288\n",
    "\n",
    "linear = torch.nn.Linear(8,32)\n",
    "print(count_parameters(linear))\n",
    "# 288\n",
    "\n",
    "print(conv.weight.shape)\n",
    "print(conv.bias.shape)\n",
    "# torch.Size([32, 8, 1])\n",
    "print(linear.weight.shape)\n",
    "print(linear.bias.shape)\n",
    "# torch.Size([32, 8])\n",
    "\n",
    "# use same initialization\n",
    "linear.weight = torch.nn.Parameter(conv.weight.squeeze(2))\n",
    "linear.bias = torch.nn.Parameter(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94cac80b40987d9dfcd7d9664f83c8b2cf119f0030fe115bcc4b81bcd6645dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
