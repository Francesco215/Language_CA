{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch, math, einops\n",
    "from src import Tokenizer, linear_unidirectional_graph_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('gpt2')\n",
    "pretraied = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.GPT2 import GPT2, GPT2_Encoder, GPT2_LM_Head\n",
    "\n",
    "encoder=GPT2_Encoder()\n",
    "decoder=GPT2_LM_Head()\n",
    "\n",
    "model=GPT2(encoder, decoder, tokenizer,dropout=0)\n",
    "\n",
    "model.load_from_original(pretraied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randint(0, 50257, (1, 10))\n",
    "x=torch.randn([1,13,768])\n",
    "\n",
    "sequence_lenght=17\n",
    "batch_size=1\n",
    "n_heads=13\n",
    "d_Embedding=64*n_heads\n",
    "Q=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "K=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "V=torch.randn([1,n_heads,sequence_lenght,d_Embedding//n_heads])\n",
    "\n",
    "\n",
    "out, att = pretraied.transformer.h[0].attn._attn(Q,K,V,None,None)\n",
    "#out=out.permute(1, 0, 2).contiguous().view(batch_size, sequence_lenght, d_Embedding)\n",
    "out.shape,att.shape\n",
    "out=out.permute(0,2,1,3).view(sequence_lenght,n_heads,d_Embedding//n_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_pretrained = einops.einsum(Q,K,'... s e, ... t e -> ... s t')/math.sqrt(K.size(-1))\n",
    "alignments=att_pretrained\n",
    "query_length, key_length = Q.size(-2), K.size(-2)\n",
    "\n",
    "causal_mask = pretraied.transformer.h[0].attn.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n",
    "mask_value = -torch.tensor(float('inf'))\n",
    "att_pretrained = torch.where(causal_mask, att_pretrained, mask_value)\n",
    "alignments=att_pretrained\n",
    "att_pretrained = torch.nn.functional.softmax(att_pretrained, dim=-1)\n",
    "att_pretrained = att_pretrained.view(n_heads,sequence_lenght,sequence_lenght)\n",
    "\n",
    "(att_pretrained==att).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4194,  0.1568, -0.0189,  1.6448, -0.9277, -0.5320,  0.4555, -0.7678,\n",
      "         0.9130, -0.1805,  0.0130, -0.5781, -1.9383])\n",
      "tensor([ 0.4194,  0.1568, -0.0189,  1.6448, -0.9277, -0.5320,  0.4555, -0.7678,\n",
      "         0.9130, -0.1805,  0.0130, -0.5781, -1.9383])\n"
     ]
    }
   ],
   "source": [
    "from src.transformerMP import attention_message\n",
    "\n",
    "graph_maker=linear_unidirectional_graph_maker(40)\n",
    "\n",
    "edge_index=graph_maker(sequence_lenght)\n",
    "senders, receivers = edge_index\n",
    "\n",
    "Q1=Q.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "K1=K.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "V1=V.view(n_heads,sequence_lenght,d_Embedding//n_heads).permute(1,0,2)\n",
    "\n",
    "#o,a=attention_message(Q,K,V,edge_index,0)\n",
    "#o.view(sequence_lenght,d_Embedding)\n",
    "\n",
    "N=20\n",
    "\n",
    "i,j=receivers[N],senders[N]\n",
    "print((Q1[receivers]*K1[senders]).sum(dim=-1)[N]/math.sqrt(K.size(-1)))\n",
    "\n",
    "print(alignments.view(n_heads,sequence_lenght,sequence_lenght)[:,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformerMP import normalize_strength\n",
    "from math import sqrt\n",
    "def attention_message(Q:torch.Tensor,\n",
    "                      K:torch.Tensor,\n",
    "                      V:torch.Tensor,\n",
    "                      edge_index:torch.Tensor,\n",
    "                      att_dropout=0.1\n",
    "                      ):\n",
    "\n",
    "    senders, receivers = edge_index\n",
    "    N,h,d=K.shape   \n",
    "\n",
    "    #Q.K^T \n",
    "    att=(Q[receivers]*K[senders]).sum(dim=-1)/sqrt(d)\n",
    "\n",
    "    #softmax    \n",
    "    att = torch.exp(att+3-att.max()) #could be done in-plase using the function att.exp_() if memory is a bootleneck\n",
    "    attention = normalize_strength(att, receivers, N, h)\n",
    "\n",
    "    #Dropout\n",
    "    #att=attention_dropout(attention, att_dropout)\n",
    "\n",
    "    #softmax*V\n",
    "    att = einops.einsum(attention,V[senders],' ... , ... c -> ... c')\n",
    "    out = torch.zeros_like(V,device=V.device)\n",
    "\n",
    "    return out.index_add(0,receivers,att), attention #could be done in-place using the function out.index_add_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5],\n",
      "        [5, 5, 5, 5, 5, 5]])\n",
      "tensor([0.5131, 0.2150, 0.1793, 0.0338, 0.0323, 0.0166, 0.1188, 0.2253, 0.0064,\n",
      "        0.0565, 0.0360, 0.1415])\n",
      "tensor([0.5131, 0.2150, 0.1793, 0.0338, 0.0323, 0.0166, 0.1188, 0.2253, 0.0064,\n",
      "        0.0565, 0.0360, 0.1415])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_function,a=attention_message(Q1,K1,V1,edge_index,0)\n",
    "\n",
    "i=3\n",
    "print(edge_index[:,receivers==j])\n",
    "print(att_pretrained[i,j:,j])\n",
    "print(a[senders==j,i])\n",
    "asd=True\n",
    "for j in range(att_pretrained.shape[-1]):\n",
    "    asd= asd and torch.allclose(att_pretrained[:,j:,j],a.permute(1,0)[:,senders==j],1e-4,1e-4)\n",
    "asd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 17, 17]) torch.Size([13, 17, 64])\n",
      "torch.Size([13, 17, 17, 64])\n",
      "torch.Size([153, 13]) torch.Size([153, 13, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "V=V.view(n_heads,sequence_lenght,d_Embedding//n_heads)\n",
    "att_pretrained.shape,V.shape\n",
    "print(att_pretrained.shape,V.shape)\n",
    "AV_pretrained=einops.einsum(att_pretrained,V,'h i j, h j e -> h i j e')\n",
    "print(AV_pretrained.shape)\n",
    "\n",
    "if not (AV_pretrained.sum(-2)==torch.matmul(att_pretrained,V)).all(): #true\n",
    "    print (False)\n",
    "\n",
    "print(a.shape,V1[senders].shape)\n",
    "dsa=einops.einsum(a,V1[senders],'..., ... e -> ... e')\n",
    "dsa.shape==V1[senders].shape==(152, n_heads, d_Embedding//n_heads) #True\n",
    "\n",
    "N=9\n",
    "i,j=senders[N],receivers[N]\n",
    "print(torch.allclose(dsa[N],AV_pretrained[:,j,i],1e-3,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 13, 64]) torch.Size([17, 13, 64])\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pretrained=AV_pretrained.sum(-2).permute(1,0,2)\n",
    "dsum=torch.zeros_like(V1).index_add(0,receivers,dsa)\n",
    "\n",
    "print(out_pretrained.shape,dsum.shape)\n",
    "print(torch.allclose(out_pretrained,dsum,1e-3,1e-3))\n",
    "torch.allclose(out,out_function,1e-3,1e-3)\n",
    "#print((dsa[N]==asd[:,j,i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aasdasda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=13\n",
    "heads=12\n",
    "d_Embedding=64*heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "def merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)\n",
    "\n",
    "def original_c_attn(x,i=0):\n",
    "        query, key, value = pretraied.transformer.h[i].attn.c_attn(x).split(d_Embedding, dim=2)\n",
    "\n",
    "        dim=d_Embedding//heads\n",
    "        query = split_heads(query, heads, dim)\n",
    "        key = split_heads(key, heads, dim)\n",
    "        value = split_heads(value, heads, dim)\n",
    "\n",
    "        return query,key,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=torch.randn([1,sequence_length,d_Embedding])\n",
    "\n",
    "edge_index=graph_maker(13)\n",
    "Qp,Kp,Vp=original_c_attn(x)\n",
    "Q,K,P=model.transformer_blocks[0].attention_block.make_QKV(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 13, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-10.1809,   1.7356,   1.2191,  17.4904,  -9.6279,   7.9092,   1.9963,\n",
       "          -7.3492,   0.7449,  -1.1058,   0.9803,  -8.7451,  -7.9611],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([-10.1809,   1.7356,   1.2191,  17.4904,  -9.6279,   7.9092,   1.9963,\n",
       "          -7.3492,   0.7449,  -1.1058,   0.9803,  -8.7451,  -7.9611],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QQ=Qp.view([heads,sequence_length,d_Embedding//heads]).permute(1,0,2)\n",
    "\n",
    "QQ[:,0,0],Q[:,0,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94cac80b40987d9dfcd7d9664f83c8b2cf119f0030fe115bcc4b81bcd6645dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
