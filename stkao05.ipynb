{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed7f10b8-a29e-41a0-8e63-3197c2da5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fd4b4b0-215a-42c5-9c38-c7b0205e2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/shakespeare_data/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c129864-d532-4354-b8d1-7932303e93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#     device = torch.device(\"mps\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2989a65c-b1f9-4435-9c04-ca6becac1df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = open('shakespeare_data/input.txt', 'r').read()\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a7c69b6-e82a-4552-a375-23148322cd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b5205e-3f77-4398-95f5-ade916e3aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(txt))\n",
    "chars.sort()\n",
    "\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for i, c in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67005dad-d7b6-49d3-865f-81786ecf25b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111539)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i =  math.floor(0.9 * len(txt))\n",
    "train_txt = txt[0:i]\n",
    "valid_txt = txt[i+1:]\n",
    "\n",
    "len(train_txt), len(valid_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c64f857-1ffb-4db0-9020-23912e76b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tkns = [ctoi[c] for c in train_txt]\n",
    "valid_tkns = [ctoi[c] for c in valid_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd1b8340-19fb-4300-9387-334a48a44e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "block_size = 64\n",
    "\n",
    "def txt_to_token(t):\n",
    "    return [ctoi[c] for c in t]\n",
    "\n",
    "\n",
    "# (B, L)\n",
    "def random_batch(split=\"train\"):\n",
    "    data = train_tkns if split == \"train\" else valid_tkns\n",
    "    \n",
    "    i = randint(0, len(data)-block_size-1)\n",
    "    x = torch.tensor(data[i:i+block_size], device=device)\n",
    "    y = torch.tensor(data[i+1:i+block_size+1], device=device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = random_batch(\"train\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "693304ce-a986-4098-bfc6-3f2839ca7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model,n_iter=10):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for split in [\"train\", \"valid\"]:   \n",
    "        loss=0\n",
    "        for _ in range(n_iter):     \n",
    "            x, y = random_batch(split)\n",
    "            logits = model(x) # (L, C)\n",
    "            #L, C = logits.shape\n",
    "            loss+= F.cross_entropy(logits, y)\n",
    "        losses.append(loss.item()/n_iter)\n",
    "\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0aa918a-b6fc-4c73-a2fc-6c87c01450c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model):\n",
    "    model.eval()\n",
    "\n",
    "    max_len = 500\n",
    "    tks = [0]*block_size\n",
    "\n",
    "    for i in range(max_len):\n",
    "        ctx = torch.tensor(tks[i:i+block_size]) # (L)\n",
    "        ctx = ctx.view(-1) # (L)\n",
    "\n",
    "        logits = model(ctx) # (L, C)\n",
    "        probs = F.softmax(logits, dim=-1) # (L, C)\n",
    "        probs = probs[-1,:] # (C), # the last in the sequence is the newly generated\n",
    "        yi = torch.multinomial(probs, 1)\n",
    "        tks.append(yi.item())\n",
    "\n",
    "    tks = tks[block_size:]\n",
    "    chars = [itoc[t] for t in tks]\n",
    "    model.train()\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5836874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "def rotary_encoding(x, base=1e-5, thetas=None):\n",
    "    \"\"\"Applies a rotary embedding to a tensor.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Tensor to apply the rotary embedding to.\n",
    "            x.shape=(sequence_lenght, n_heads, d_embedding)\n",
    "        base (float, optional): Base of the logarithm. Defaults to 1e-5.\n",
    "        thetas (torch.Tensor, optional): Tensor containing the thetas.\n",
    "            It can be used in case you want to apply learned positional encoding.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor with the rotary embedding applied.\n",
    "    \"\"\"\n",
    "\n",
    "    #pad with zeros if odd, otherwise we cant pair up consecutive elements\n",
    "    odd = False\n",
    "    if x.shape[0] % 2 != 0:\n",
    "        zeros = torch.zeros((1, *x.shape[1:]), device=x.device)\n",
    "        x = torch.cat([x, zeros], dim=0)\n",
    "        odd = True\n",
    "\n",
    "    #pair up consecutive elements\n",
    "    x1 = einops.rearrange(x, '(n1 n2) ... -> n1 n2 ...', n2=2)\n",
    "\n",
    "    #pair up elements and swap them\n",
    "    x2 = x1[:, torch.tensor([1, 0])]\n",
    "    x2[:, 0] = -x2[:, 0]\n",
    "\n",
    "    #create phases\n",
    "    sin, cos = make_sin_cos(x1.shape, base, thetas, device=x.device)\n",
    "\n",
    "    #apply rotation\n",
    "    x1 = einops.einsum(x1, cos, 'n ... c, n c -> n ... c')\n",
    "    x2 = einops.einsum(x2, sin, 'n ... c, n c -> n ... c')\n",
    "    x = x1+x2\n",
    "    x = einops.rearrange(x, 'n1 n2 ... -> (n1 n2) ...', n2=2)\n",
    "\n",
    "    if odd:\n",
    "        return x[:-1]  # remove padding if odd\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_sin_cos(shape, base=1e-5, thetas=None, device='cpu'):\n",
    "\n",
    "    if thetas is None:\n",
    "        thetas = torch.logspace(0, 1, shape[-1], base, device=device)\n",
    "    indices = torch.arange(0, shape[0], device=device)\n",
    "    phases = einops.einsum(indices, thetas, 'a, c -> a c')\n",
    "\n",
    "    #rotate\n",
    "    sin = torch.sin(phases)\n",
    "    cos = torch.cos(phases)\n",
    "\n",
    "    #zetas = torch.linspace(gamma,2*x.shape[-1]+gamma,x.shape[-1],device=x.device)/(1+gamma)\n",
    "\n",
    "    return sin, cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c97c14d-131a-4e56-bf3b-51f2ff9bc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return (L, C)\n",
    "def pos_encoding(x):\n",
    "    L, C = x.shape\n",
    "    pos = torch.arange(0, L).view(-1, 1) # (L, 1)\n",
    "    div = 2 * torch.arange(0, C) / C # (C)\n",
    "    div = torch.pow(10000, div) # (C)\n",
    "    e = pos / div\n",
    "    pe = torch.zeros(L, C)\n",
    "    pe[:,0::2] = torch.sin(e[:,0::2])\n",
    "    pe[:,1::2] = torch.cos(e[:,1::2])\n",
    "    \n",
    "    pe = pe.to(device)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "683fa87d-e2d3-44f4-b02e-2f608cedf936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 7])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttension(nn.Module):    \n",
    "    \n",
    "    def __init__(self, head_num, head_size, in_size, out_size, rotary_encoding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        self.head_num = head_num        \n",
    "        self.attn = nn.Linear(in_size, 3 * head_num * head_size, bias=False)\n",
    "        self.ffn = nn.Linear(head_num * head_size, out_size, bias=False)\n",
    "        self.rotary_encoding = rotary_encoding\n",
    "\n",
    "        \n",
    "    # x: (L, C)  \n",
    "    # return: (L, C')\n",
    "    def forward(self, x):\n",
    "        L, C = x.shape\n",
    "        \n",
    "        z = self.attn(x) # (L, 3 * hn * hs)\n",
    "        k, q, v = torch.split(z, self.head_num * self.head_size, dim=-1) # (L, hn * hs)\n",
    "\n",
    "        # reshape the output to have the correct shape\n",
    "        q = q.view(L, self.head_num, self.head_size)\n",
    "        k = k.view(L, self.head_num, self.head_size)\n",
    "        v = v.view(L, self.head_num, self.head_size)\n",
    "\n",
    "        # apply rotary encoding if needed\n",
    "        if self.rotary_encoding:\n",
    "            q = rotary_encoding(q)\n",
    "            k = rotary_encoding(k)\n",
    "        \n",
    "        q=q.permute(1,0,2) # ( hn, L, hs)\n",
    "        k=k.permute(1,0,2)\n",
    "        v=v.permute(1,0,2)\n",
    "\n",
    "        \n",
    "        q = q.permute(0, 2, 1) # ( hn, hs, L)\n",
    "        attn = (k @ q) / self.head_size**0.5 # (hn, L, L)\n",
    "        mask = torch.tril(torch.ones(L, L)) == 0\n",
    "        mask = mask.to(device)\n",
    "        attn = attn.masked_fill(mask, -float('inf')) # (B, hn, L, L)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        y = attn @ v # (hn, L, hs)\n",
    "        y = y.permute(1, 0, 2) # (L, hn, hs)\n",
    "        y = y.contiguous().view(L, -1) # (L, hn * hs)\n",
    "        y = self.ffn(y) # (L, C)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "        \n",
    "x = torch.randn(block_size, 9) # (L, C)\n",
    "x = x.to(device)\n",
    "mh = MultiHeadAttension(5, 3, 9, 7)\n",
    "mh = mh.to(device)\n",
    "mh(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a9dc7d-67b8-4560-b673-acb36c69402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_size, out_size)\n",
    "        self.linear2 = nn.Linear(out_size, out_size)\n",
    "    \n",
    "    # (B, L, C)\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = torch.relu(y)\n",
    "        y = self.linear2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5f8c216-35b6-4cde-94af-781144ea494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_rotary=False\n",
    "\n",
    "\n",
    "class Block(nn.Module):    \n",
    "    \n",
    "    def __init__(self, emb_size, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert emb_size % head_size == 0\n",
    "        head_num = emb_size // head_size\n",
    "        \n",
    "        self.mha = MultiHeadAttension(head_num, \n",
    "                                      head_size, \n",
    "                                      in_size=emb_size, \n",
    "                                      out_size=emb_size,\n",
    "                                      rotary_encoding=use_rotary)\n",
    "        self.lnorm1 = nn.LayerNorm(emb_size)\n",
    "        self.lnorm2 = nn.LayerNorm(emb_size)\n",
    "        self.ffn = MLP(emb_size, emb_size)\n",
    "        \n",
    "        \n",
    "    # x: (B, L, emb)\n",
    "    def forward(self, x):\n",
    "        y = self.mha(x) + x\n",
    "        y = self.lnorm1(y)\n",
    "        y = self.ffn(y) + y\n",
    "        y = self.lnorm2(y)\n",
    "        return y\n",
    "    \n",
    "# x = torch.randn(3, 4, 10)\n",
    "# b = Block(10, 2)\n",
    "# b(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63d81641-f331-4341-822d-eeada13a50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 128\n",
    "head_size = 32\n",
    "\n",
    "class Transformer(nn.Module):    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(emb_size, head_size),\n",
    "            Block(emb_size, head_size),\n",
    "        )\n",
    "        self.linear = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    # (L) -> (L, C)\n",
    "    def forward(self, x):\n",
    "        y = self.embed(x) # (L, emb)\n",
    "        if not use_rotary:\n",
    "            y = y + pos_encoding(y) # (L, emb)\n",
    "        y = self.blocks(y) # (L, emb)\n",
    "        y = self.linear(y) # (L, vocab)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4d0adfe-9fdc-4bba-9ef0-d112cd0b2097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameter: 214849\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model = model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "count = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"total parameter: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17306edf-1569-40b2-96f9-6b2e39db2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0/80000: 4.4375  4.3993\n",
      "  500/80000: 2.8487  2.7606\n",
      " 1000/80000: 2.6795  2.6591\n",
      " 1500/80000: 2.6022  2.5743\n",
      " 2000/80000: 2.5221  2.6714\n",
      " 2500/80000: 2.5135  2.5218\n",
      " 3000/80000: 2.4672  2.4712\n",
      " 3500/80000: 2.3915  2.3813\n",
      " 4000/80000: 2.4754  2.3842\n",
      " 4500/80000: 2.3706  2.3820\n",
      " 5000/80000: 2.3130  2.4805\n",
      " 5500/80000: 2.3531  2.3642\n",
      " 6000/80000: 2.2285  2.2691\n",
      " 6500/80000: 2.3126  2.3828\n",
      " 7000/80000: 2.3603  2.4330\n",
      " 7500/80000: 2.1525  2.2562\n",
      " 8000/80000: 2.3713  2.2203\n",
      " 8500/80000: 2.2481  2.2843\n",
      " 9000/80000: 2.1526  2.2442\n",
      " 9500/80000: 2.1602  2.1818\n",
      "10000/80000: 2.2077  2.2019\n",
      "10500/80000: 2.3275  2.2109\n",
      "11000/80000: 2.1323  2.2588\n",
      "11500/80000: 2.1680  2.2122\n",
      "12000/80000: 2.0331  2.2068\n",
      "12500/80000: 2.1375  2.1906\n",
      "13000/80000: 2.1628  2.1230\n",
      "13500/80000: 2.2217  2.2068\n",
      "14000/80000: 2.1822  2.1622\n",
      "14500/80000: 2.1279  2.1909\n",
      "15000/80000: 2.1791  2.1814\n",
      "15500/80000: 2.0448  2.1582\n",
      "16000/80000: 2.0736  2.1378\n",
      "16500/80000: 2.1359  2.1472\n",
      "17000/80000: 2.0423  2.1196\n",
      "17500/80000: 2.0780  2.1314\n",
      "18000/80000: 2.0930  2.1098\n",
      "18500/80000: 1.9956  2.0948\n",
      "19000/80000: 2.0054  2.0600\n",
      "19500/80000: 2.0489  2.1164\n",
      "20000/80000: 2.0518  2.1065\n",
      "20500/80000: 2.0332  2.0378\n",
      "21000/80000: 2.0138  2.1590\n",
      "21500/80000: 2.1062  2.0128\n",
      "22000/80000: 1.8930  1.9708\n",
      "22500/80000: 2.1199  1.9713\n",
      "23000/80000: 2.0836  2.0442\n",
      "23500/80000: 1.9318  2.1472\n",
      "24000/80000: 2.0142  2.0361\n",
      "24500/80000: 1.9650  2.0983\n",
      "25000/80000: 2.0009  2.1577\n",
      "25500/80000: 1.8066  2.0801\n",
      "26000/80000: 1.9989  2.0400\n",
      "26500/80000: 1.9701  2.0292\n",
      "27000/80000: 1.9350  2.0555\n",
      "27500/80000: 1.9275  2.0985\n",
      "28000/80000: 1.8547  2.0612\n",
      "28500/80000: 1.9563  2.0113\n",
      "29000/80000: 1.9098  2.0357\n",
      "29500/80000: 1.9113  2.1027\n",
      "30000/80000: 2.0563  2.1633\n",
      "30500/80000: 1.8453  1.9916\n",
      "31000/80000: 1.9480  2.0874\n",
      "31500/80000: 1.9819  2.0870\n",
      "32000/80000: 1.9610  2.0131\n",
      "32500/80000: 1.8657  1.9888\n",
      "33000/80000: 1.8710  1.9462\n",
      "33500/80000: 1.8430  2.0759\n",
      "34000/80000: 2.0093  2.0187\n",
      "34500/80000: 1.9136  1.8527\n",
      "35000/80000: 1.9399  1.9069\n",
      "35500/80000: 1.9280  1.9594\n",
      "36000/80000: 1.9928  1.9570\n",
      "36500/80000: 1.8757  2.0057\n",
      "37000/80000: 1.8155  2.1012\n",
      "37500/80000: 1.8220  2.0077\n",
      "38000/80000: 1.9627  1.9611\n",
      "38500/80000: 1.8516  1.9333\n",
      "39000/80000: 1.7116  2.0394\n",
      "39500/80000: 1.7438  1.9633\n",
      "40000/80000: 1.8690  2.0864\n",
      "40500/80000: 2.0000  2.1220\n",
      "41000/80000: 1.8427  2.0433\n",
      "41500/80000: 1.8451  1.8524\n",
      "42000/80000: 1.9474  2.0326\n",
      "42500/80000: 1.8220  1.9872\n",
      "43000/80000: 1.8049  1.9560\n",
      "43500/80000: 1.8967  1.9804\n",
      "44000/80000: 1.7422  2.0370\n",
      "44500/80000: 1.7818  1.8755\n",
      "45000/80000: 1.7082  1.9755\n",
      "45500/80000: 1.8552  1.9504\n",
      "46000/80000: 1.7600  2.0495\n",
      "46500/80000: 1.8218  1.9582\n",
      "47000/80000: 1.9564  1.8744\n",
      "47500/80000: 1.7353  2.0593\n",
      "48000/80000: 1.8079  1.8092\n",
      "48500/80000: 1.7090  2.0805\n",
      "49000/80000: 1.8262  1.9224\n",
      "49500/80000: 1.7953  2.1000\n",
      "50000/80000: 1.7930  1.9367\n",
      "50500/80000: 1.8376  1.8930\n",
      "51000/80000: 1.8023  1.9694\n",
      "51500/80000: 1.6491  1.9664\n",
      "52000/80000: 1.6752  1.8492\n",
      "52500/80000: 1.6775  1.9244\n",
      "53000/80000: 1.8464  2.0046\n",
      "53500/80000: 1.7269  1.9307\n",
      "54000/80000: 1.7076  1.8963\n",
      "54500/80000: 1.7662  2.0339\n",
      "55000/80000: 1.7673  1.9515\n",
      "55500/80000: 1.8495  1.8712\n",
      "56000/80000: 1.7061  1.9399\n",
      "56500/80000: 1.6603  1.7885\n",
      "57000/80000: 1.7246  1.8884\n",
      "57500/80000: 1.6208  1.9908\n",
      "58000/80000: 1.7219  1.9129\n",
      "58500/80000: 1.7484  1.8633\n",
      "59000/80000: 1.7466  1.8813\n",
      "59500/80000: 1.8561  1.9753\n",
      "60000/80000: 1.8949  1.7896\n",
      "60500/80000: 1.7321  1.9247\n",
      "61000/80000: 1.7869  1.9264\n",
      "61500/80000: 1.6344  1.9659\n",
      "62000/80000: 1.7222  1.9058\n",
      "62500/80000: 1.8419  1.7942\n",
      "63000/80000: 1.6070  1.8866\n",
      "63500/80000: 1.6877  2.1356\n",
      "64000/80000: 1.7558  1.7379\n",
      "64500/80000: 1.6563  1.9707\n",
      "65000/80000: 1.6783  1.7278\n",
      "65500/80000: 1.6602  1.9825\n",
      "66000/80000: 1.6836  1.8833\n",
      "66500/80000: 1.6212  1.9976\n",
      "67000/80000: 1.8081  1.9914\n",
      "67500/80000: 1.8712  1.7001\n",
      "68000/80000: 1.8026  1.8842\n",
      "68500/80000: 1.8493  1.9402\n",
      "69000/80000: 1.7803  1.8308\n",
      "69500/80000: 1.5890  1.8397\n",
      "70000/80000: 1.7446  2.1165\n",
      "70500/80000: 1.6953  1.8749\n",
      "71000/80000: 1.7585  1.8692\n",
      "71500/80000: 1.6763  1.9382\n",
      "72000/80000: 1.8127  1.8377\n",
      "72500/80000: 1.5705  1.8808\n",
      "73000/80000: 1.7612  1.8369\n",
      "73500/80000: 1.7026  1.8748\n",
      "74000/80000: 1.6818  1.9081\n",
      "74500/80000: 1.6801  1.8873\n",
      "75000/80000: 1.6193  1.6634\n",
      "75500/80000: 1.7664  1.8732\n",
      "76000/80000: 1.7469  2.0527\n",
      "76500/80000: 1.6938  1.8419\n",
      "77000/80000: 1.7251  1.9088\n",
      "77500/80000: 1.7039  1.8463\n",
      "78000/80000: 1.6988  1.8096\n",
      "78500/80000: 1.6834  1.8929\n",
      "79000/80000: 1.6904  1.7812\n",
      "79500/80000: 1.5593  1.8756\n",
      "79999/80000: 1.6577  1.8069\n"
     ]
    }
   ],
   "source": [
    "epoch = 80000\n",
    "eval_interval = 500\n",
    "eval_size = 500\n",
    "lossi = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    xb, yb = random_batch()\n",
    "    logits = model(xb) # (L, C)\n",
    "\n",
    "    L, C = logits.shape\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i % eval_interval == 0 or i == epoch-1:\n",
    "        tr, va = estimate_loss(model)\n",
    "        lossi.append((tr, va))\n",
    "        print(f\"{i:5d}/{epoch}: {tr:.4f}  {va:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7f33110-faa7-4140-aa76-5f31d92c1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1.7554\n",
      "valid: 1.7840\n"
     ]
    }
   ],
   "source": [
    "tr_loss, va_loss = estimate_loss(model)\n",
    "\n",
    "print(f\"train: {tr_loss:.4f}\")\n",
    "print(f\"valid: {va_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12c5665e-27df-40e3-8f1c-f6f53a0916da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAUTOLYCU S:\n",
      "To thilers as patigar:\n",
      "Leake an and three; beggaren'roy, and firttione.\n",
      "\n",
      "ChORTER:\n",
      "Wethen leave dembte bidineds mavoreld this:\n",
      "Me but him dolloook, of, grow\n",
      "In wile begfore that iggn: dis,\n",
      "Sheir With honer, or awer's pamords of in the honourse.\n",
      "\n",
      "CORIOLANUS:\n",
      "O, be my garow an tome know nexclius makest be made;\n",
      "MadddO kingstran'st hanger forceds with he form apoialop; 'twit he stray for theice\n",
      "is olly spirage my man; and iftly may beide:\n",
      "Say wore her; I am I fare men be\n",
      "ave steenged my\n"
     ]
    }
   ],
   "source": [
    "print(sample(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197fd44-c758-4e9f-b9ac-3a3acb19e7ad",
   "metadata": {},
   "source": [
    "## Log\n",
    "\n",
    "- Bi-gram: 2.4716, 2.4755\n",
    "- Single-head attention: 2.3899, 2.4041\n",
    "- Multi-head attention, single layer: 2.0820, 2.1165\n",
    "- Multi-head attention, single layer, positional encoding: 1.8575, 1.9216\n",
    "- 2-layer transformer (with everything, MHA, positional encoding, layer norm): 1.7155, 1.7952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
