{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7f10b8-a29e-41a0-8e63-3197c2da5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd4b4b0-215a-42c5-9c38-c7b0205e2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/shakespeare_data/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c129864-d532-4354-b8d1-7932303e93ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#     device = torch.device(\"mps\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2989a65c-b1f9-4435-9c04-ca6becac1df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = open('shakespeare_data/input.txt', 'r').read()\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7c69b6-e82a-4552-a375-23148322cd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b5205e-3f77-4398-95f5-ade916e3aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(txt))\n",
    "chars.sort()\n",
    "\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for i, c in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67005dad-d7b6-49d3-865f-81786ecf25b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111539)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i =  math.floor(0.9 * len(txt))\n",
    "train_txt = txt[0:i]\n",
    "valid_txt = txt[i+1:]\n",
    "\n",
    "len(train_txt), len(valid_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c64f857-1ffb-4db0-9020-23912e76b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tkns = [ctoi[c] for c in train_txt]\n",
    "valid_tkns = [ctoi[c] for c in valid_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1b8340-19fb-4300-9387-334a48a44e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "block_size = 64\n",
    "\n",
    "def txt_to_token(t):\n",
    "    return [ctoi[c] for c in t]\n",
    "\n",
    "\n",
    "# (B, L)\n",
    "def random_batch(split=\"train\"):\n",
    "    data = train_tkns if split == \"train\" else valid_tkns\n",
    "    \n",
    "    i = randint(0, len(data)-block_size-1)\n",
    "    x = torch.tensor(data[i:i+block_size], device=device)\n",
    "    y = torch.tensor(data[i+1:i+block_size+1], device=device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = random_batch(\"train\")\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "693304ce-a986-4098-bfc6-3f2839ca7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model,n_iter=10):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for split in [\"train\", \"valid\"]:   \n",
    "        loss=0\n",
    "        for _ in range(n_iter):     \n",
    "            x, y = random_batch(split)\n",
    "            logits = model(x) # (L, C)\n",
    "            #L, C = logits.shape\n",
    "            loss+= F.cross_entropy(logits, y)\n",
    "        losses.append(loss.item()/n_iter)\n",
    "\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0aa918a-b6fc-4c73-a2fc-6c87c01450c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model):\n",
    "    model.eval()\n",
    "\n",
    "    max_len = 500\n",
    "    tks = [0]*block_size\n",
    "\n",
    "    for i in range(max_len):\n",
    "        ctx = torch.tensor(tks[i:i+block_size]) # (L)\n",
    "        ctx = ctx.view(-1) # (L)\n",
    "\n",
    "        logits = model(ctx) # (L, C)\n",
    "        probs = F.softmax(logits, dim=-1) # (L, C)\n",
    "        probs = probs[-1,:] # (C), # the last in the sequence is the newly generated\n",
    "        yi = torch.multinomial(probs, 1)\n",
    "        tks.append(yi.item())\n",
    "\n",
    "    tks = tks[block_size:]\n",
    "    chars = [itoc[t] for t in tks]\n",
    "    model.train()\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8089ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nundefined value rotary_pos_emb:\n  File \"/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_84182/3611785811.py\", line 30\n@torch.jit.script\ndef apply_rotary_pos_emb(q, k):\n    cos, sin = rotary_pos_emb(q)\n               ~~~~~~~~~~~~~~ <--- HERE\n    q_rot = (q * cos) + (rotate_half(q) * sin)\n    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_84182/3611785811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mq_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_rcb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_rcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateResolutionCallbackFromClosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         fn = torch._C._jit_script_compile(\n\u001b[0m\u001b[1;32m   1344\u001b[0m             \u001b[0mqualified_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_default_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nundefined value rotary_pos_emb:\n  File \"/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_84182/3611785811.py\", line 30\n@torch.jit.script\ndef apply_rotary_pos_emb(q, k):\n    cos, sin = rotary_pos_emb(q)\n               ~~~~~~~~~~~~~~ <--- HERE\n    q_rot = (q * cos) + (rotate_half(q) * sin)\n    k_rot = (k * cos) + (rotate_half(k) * sin)\n"
     ]
    }
   ],
   "source": [
    "class Rotary(torch.nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x, seq_dim=0):\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dim],device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "            self.cos_cached = emb.cos()[:, None, :]\n",
    "            self.sin_cached = emb.sin()[:, None, :]\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "\n",
    "# rotary pos emb helpers:\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(q, k):\n",
    "    cos, sin = rotary_pos_emb(q)\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "\n",
    "seq_len = 61\n",
    "n_heads = 4\n",
    "head_dim = 32\n",
    "q = torch.randn(seq_len, n_heads, head_dim)\n",
    "k = torch.randn(seq_len, n_heads, head_dim)\n",
    "\n",
    "# define a rotary positional embedding layer\n",
    "rotary_pos_emb = Rotary(head_dim)\n",
    "\n",
    "# apply the rotary positional embedding to the q and k vectors\n",
    "q_rot, k_rot = apply_rotary_pos_emb(q, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259e5bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apply_rotary_pos_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_82519/1200729922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# apply the rotary positional embedding to the q and k vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mq_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'apply_rotary_pos_emb' is not defined"
     ]
    }
   ],
   "source": [
    "seq_len = 61\n",
    "n_heads = 4\n",
    "head_dim = 32\n",
    "q = torch.randn(seq_len, n_heads, head_dim)\n",
    "k = torch.randn(seq_len, n_heads, head_dim)\n",
    "\n",
    "# define a rotary positional embedding layer\n",
    "rotary_pos_emb = Rotary(head_dim)\n",
    "\n",
    "# apply the rotary positional embedding to the q and k vectors\n",
    "q_rot, k_rot = apply_rotary_pos_emb(q, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c97c14d-131a-4e56-bf3b-51f2ff9bc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return (L, C)\n",
    "def pos_encoding(x):\n",
    "    L, C = x.shape\n",
    "    pos = torch.arange(0, L).view(-1, 1) # (L, 1)\n",
    "    div = 2 * torch.arange(0, C) / C # (C)\n",
    "    div = torch.pow(10000, div) # (C)\n",
    "    e = pos / div\n",
    "    pe = torch.zeros(L, C)\n",
    "    pe[:,0::2] = torch.sin(e[:,0::2])\n",
    "    pe[:,1::2] = torch.cos(e[:,1::2])\n",
    "    \n",
    "    pe = pe.to(device)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "683fa87d-e2d3-44f4-b02e-2f608cedf936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.encoder import rotary_encoding\n",
    "\n",
    "class MultiHeadAttension(nn.Module):    \n",
    "    \n",
    "    def __init__(self, head_num, head_size, in_size, out_size, rotary_encoding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        self.head_num = head_num        \n",
    "        self.attn = nn.Linear(in_size, 3 * head_num * head_size, bias=False)\n",
    "        self.ffn = nn.Linear(head_num * head_size, out_size, bias=False)\n",
    "        self.rotary_encoding = rotary_encoding\n",
    "\n",
    "        \n",
    "    # x: (L, C)  \n",
    "    # return: (L, C')\n",
    "    def forward(self, x):\n",
    "        L, C = x.shape\n",
    "        \n",
    "        z = self.attn(x) # (L, 3 * hn * hs)\n",
    "        k, q, v = torch.split(z, self.head_num * self.head_size, dim=-1) # (L, hn * hs)\n",
    "\n",
    "        # reshape the output to have the correct shape\n",
    "        q = q.view(L, self.head_num, self.head_size)\n",
    "        k = k.view(L, self.head_num, self.head_size)\n",
    "        v = v.view(L, self.head_num, self.head_size)\n",
    "\n",
    "        # apply rotary encoding if needed\n",
    "        if self.rotary_encoding:\n",
    "            q = rotary_encoding(q)\n",
    "            k = rotary_encoding(k)\n",
    "\n",
    "        \n",
    "        q=q.permute(1,0,2) # ( hn, L, hs)\n",
    "        k=k.permute(1,0,2)\n",
    "        v=v.permute(1,0,2)\n",
    "\n",
    "        \n",
    "        q = q.permute(0, 2, 1) # ( hn, hs, L)\n",
    "        attn = (k @ q) / self.head_size**0.5 # (hn, L, L)\n",
    "        mask = torch.tril(torch.ones(L, L)) == 0\n",
    "        mask = mask.to(device)\n",
    "        attn = attn.masked_fill(mask, -float('inf')) # (B, hn, L, L)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        y = attn @ v # (hn, L, hs)\n",
    "        y = y.permute(1, 0, 2) # (L, hn, hs)\n",
    "        y = y.contiguous().view(L, -1) # (L, hn * hs)\n",
    "        y = self.ffn(y) # (L, C)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "        \n",
    "x = torch.randn(block_size, 9) # (L, C)\n",
    "x = x.to(device)\n",
    "mh = MultiHeadAttension(5, 3, 9, 7)\n",
    "mh = mh.to(device)\n",
    "mh(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a9dc7d-67b8-4560-b673-acb36c69402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_size, out_size)\n",
    "        self.linear2 = nn.Linear(out_size, out_size)\n",
    "    \n",
    "    # (B, L, C)\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = torch.relu(y)\n",
    "        y = self.linear2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f8c216-35b6-4cde-94af-781144ea494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_rotary=True\n",
    "\n",
    "\n",
    "class Block(nn.Module):    \n",
    "    \n",
    "    def __init__(self, emb_size, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert emb_size % head_size == 0\n",
    "        head_num = emb_size // head_size\n",
    "        \n",
    "        self.mha = MultiHeadAttension(head_num, \n",
    "                                      head_size, \n",
    "                                      in_size=emb_size, \n",
    "                                      out_size=emb_size,\n",
    "                                      rotary_encoding=use_rotary)\n",
    "        self.lnorm1 = nn.LayerNorm(emb_size)\n",
    "        self.lnorm2 = nn.LayerNorm(emb_size)\n",
    "        self.ffn = MLP(emb_size, emb_size)\n",
    "        \n",
    "        \n",
    "    # x: (B, L, emb)\n",
    "    def forward(self, x):\n",
    "        y = self.mha(x) + x\n",
    "        y = self.lnorm1(y)\n",
    "        y = self.ffn(y) + y\n",
    "        y = self.lnorm2(y)\n",
    "        return y\n",
    "    \n",
    "# x = torch.randn(3, 4, 10)\n",
    "# b = Block(10, 2)\n",
    "# b(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d81641-f331-4341-822d-eeada13a50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 128\n",
    "head_size = 32\n",
    "\n",
    "class Transformer(nn.Module):    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(emb_size, head_size),\n",
    "            Block(emb_size, head_size),\n",
    "        )\n",
    "        self.linear = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    # (L) -> (L, C)\n",
    "    def forward(self, x):\n",
    "        y = self.embed(x) # (L, emb)\n",
    "        if not use_rotary:\n",
    "            y = y + pos_encoding(y) # (L, emb)\n",
    "        y = self.blocks(y) # (L, emb)\n",
    "        y = self.linear(y) # (L, vocab)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4d0adfe-9fdc-4bba-9ef0-d112cd0b2097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameter: 214849\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model = model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "count = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"total parameter: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17306edf-1569-40b2-96f9-6b2e39db2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0/80000: 4.3529  4.3444\n",
      "  500/80000: 2.7713  2.8450\n",
      " 1000/80000: 2.6230  2.5942\n",
      " 1500/80000: 2.4577  2.4123\n",
      " 2000/80000: 2.3886  2.3992\n",
      " 2500/80000: 2.2748  2.2171\n",
      " 3000/80000: 2.2638  2.2952\n",
      " 3500/80000: 2.1692  2.2438\n",
      " 4000/80000: 2.2033  2.1929\n",
      " 4500/80000: 2.1412  2.1478\n",
      " 5000/80000: 2.0780  2.2718\n",
      " 5500/80000: 2.0798  2.1297\n",
      " 6000/80000: 2.1470  2.1623\n",
      " 6500/80000: 2.0404  2.0625\n",
      " 7000/80000: 2.0953  2.0573\n",
      " 7500/80000: 2.0496  2.0759\n",
      " 8000/80000: 1.9404  2.0527\n",
      " 8500/80000: 1.9855  2.0729\n",
      " 9000/80000: 2.1580  2.0026\n",
      " 9500/80000: 2.1106  2.0704\n",
      "10000/80000: 1.9967  1.9953\n",
      "10500/80000: 2.0391  2.1085\n",
      "11000/80000: 2.0747  2.0027\n",
      "11500/80000: 1.9446  2.0539\n",
      "12000/80000: 1.9326  2.1832\n",
      "12500/80000: 1.9245  1.9689\n",
      "13000/80000: 1.8351  2.1008\n",
      "13500/80000: 1.9117  2.0519\n",
      "14000/80000: 1.9141  2.0313\n",
      "14500/80000: 1.9876  2.0466\n",
      "15000/80000: 1.9099  2.0705\n",
      "15500/80000: 1.9833  2.0280\n",
      "16000/80000: 1.8938  2.1066\n",
      "16500/80000: 1.9482  2.0023\n",
      "17000/80000: 2.0053  2.2195\n",
      "17500/80000: 1.9146  1.9806\n",
      "18000/80000: 1.9846  1.9164\n",
      "18500/80000: 1.9070  1.9279\n",
      "19000/80000: 1.8756  1.9578\n",
      "19500/80000: 1.7609  1.8885\n",
      "20000/80000: 1.8931  2.0699\n",
      "20500/80000: 1.8974  2.1034\n",
      "21000/80000: 1.8556  1.9407\n",
      "21500/80000: 1.8589  2.0202\n",
      "22000/80000: 1.7848  1.9263\n",
      "22500/80000: 1.7903  1.8577\n",
      "23000/80000: 1.7669  2.0324\n",
      "23500/80000: 1.8284  1.9464\n",
      "24000/80000: 1.8368  1.9110\n",
      "24500/80000: 1.7829  1.9538\n",
      "25000/80000: 1.7710  1.8127\n",
      "25500/80000: 1.8255  1.9669\n",
      "26000/80000: 1.7520  1.9139\n",
      "26500/80000: 1.7258  1.9238\n",
      "27000/80000: 1.6807  1.9845\n",
      "27500/80000: 1.8267  2.0042\n",
      "28000/80000: 1.8436  1.9435\n",
      "28500/80000: 1.7886  1.9186\n",
      "29000/80000: 1.8165  1.9896\n",
      "29500/80000: 1.7329  1.9857\n",
      "30000/80000: 1.8602  1.9188\n",
      "30500/80000: 1.8808  1.8491\n",
      "31000/80000: 1.6886  1.9415\n",
      "31500/80000: 1.7739  2.0851\n",
      "32000/80000: 1.7382  1.9137\n",
      "32500/80000: 1.8137  2.0010\n",
      "33000/80000: 1.7570  2.0033\n",
      "33500/80000: 1.7427  1.8779\n",
      "34000/80000: 1.7396  1.9958\n",
      "34500/80000: 1.8823  1.9460\n",
      "35000/80000: 1.7158  1.7667\n",
      "35500/80000: 1.7714  1.9341\n",
      "36000/80000: 1.8409  1.9251\n",
      "36500/80000: 1.6802  1.9303\n",
      "37000/80000: 1.6280  1.8677\n",
      "37500/80000: 1.7506  1.8658\n",
      "38000/80000: 1.8220  1.9098\n",
      "38500/80000: 1.8563  1.9470\n",
      "39000/80000: 1.9287  1.9032\n",
      "39500/80000: 1.7747  1.9173\n",
      "40000/80000: 1.7181  1.9584\n",
      "40500/80000: 1.6731  1.7782\n",
      "41000/80000: 1.5897  1.9723\n",
      "41500/80000: 1.8014  1.9309\n",
      "42000/80000: 1.8556  1.8925\n",
      "42500/80000: 1.6809  1.8711\n",
      "43000/80000: 1.8753  1.9303\n",
      "43500/80000: 1.7554  1.8972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y0/09wrr2yx6r79sjmsdnsc_0jm0000gn/T/ipykernel_84182/1562709291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 80000\n",
    "eval_interval = 500\n",
    "eval_size = 500\n",
    "lossi = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    xb, yb = random_batch()\n",
    "    logits = model(xb) # (L, C)\n",
    "\n",
    "    L, C = logits.shape\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i % eval_interval == 0 or i == epoch-1:\n",
    "        tr, va = estimate_loss(model)\n",
    "        lossi.append((tr, va))\n",
    "        print(f\"{i:5d}/{epoch}: {tr:.4f}  {va:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7f33110-faa7-4140-aa76-5f31d92c1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1.7488\n",
      "valid: 1.8058\n"
     ]
    }
   ],
   "source": [
    "tr_loss, va_loss = estimate_loss(model)\n",
    "\n",
    "print(f\"train: {tr_loss:.4f}\")\n",
    "print(f\"valid: {va_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12c5665e-27df-40e3-8f1c-f6f53a0916da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I's Clay is lispect hand it fend queed trieghar,\n",
      "But by Clatis your brothing cale, my be with of graw meed Grother.\n",
      "For chonved, thinks worn if misle thee,\n",
      "I then\n",
      "Lord headguend, with get mither distingged. Ye't dery\n",
      "\n",
      "TRUCE IF joing my a long, I fill pey man him an what's seet,\n",
      "If anlembrot, do sin thee, everce felind at me?\n",
      "\n",
      "KINCETuSlar.\n",
      "\n",
      "SORCASTER:\n",
      "Reakes more mone of madyer me, profel a poort\n",
      "What Iarme keys.\n",
      "\n",
      "Tursen, as tricust well on fall:\n",
      "I have's with with use use over endy,\n",
      "Good hear th\n"
     ]
    }
   ],
   "source": [
    "print(sample(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197fd44-c758-4e9f-b9ac-3a3acb19e7ad",
   "metadata": {},
   "source": [
    "## Log\n",
    "\n",
    "- Bi-gram: 2.4716, 2.4755\n",
    "- Single-head attention: 2.3899, 2.4041\n",
    "- Multi-head attention, single layer: 2.0820, 2.1165\n",
    "- Multi-head attention, single layer, positional encoding: 1.8575, 1.9216\n",
    "- 2-layer transformer (with everything, MHA, positional encoding, layer norm): 1.7155, 1.7952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
